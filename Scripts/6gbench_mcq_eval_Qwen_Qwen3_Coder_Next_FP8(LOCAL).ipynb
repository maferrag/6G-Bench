{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6c65ad-1cdd-4d43-b08f-7d2d202cc22a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: torch in /opt/tljh/user/lib/python3.12/site-packages (2.9.1+cu130)\n",
      "Requirement already satisfied: sglang>=0.5.8 in ./.local/lib/python3.12/site-packages (from sglang[all]>=0.5.8) (0.5.8.post1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.12/site-packages (from transformers) (0.36.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: psutil in /opt/tljh/user/lib/python3.12/site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: setuptools in /opt/tljh/user/lib/python3.12/site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (9.13.0.50)\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (13.0.0.19)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (12.0.0.15)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (12.0.3.29)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (12.6.2.49)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (2.27.7)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (3.3.24)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (13.0.39)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (13.0.39)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (1.15.0.42)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.12/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: IPython in /opt/tljh/user/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (9.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (3.9.5)\n",
      "Requirement already satisfied: apache-tvm-ffi<0.2,>=0.1.5 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.1.8.post2)\n",
      "Requirement already satisfied: anthropic>=0.20.0 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.79.0)\n",
      "Requirement already satisfied: blobfile==3.0.0 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (3.0.0)\n",
      "Requirement already satisfied: build in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.4.0)\n",
      "Requirement already satisfied: compressed-tensors in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.13.0)\n",
      "Requirement already satisfied: cuda-python==12.9 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (12.9.0)\n",
      "Requirement already satisfied: decord2 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (3.0.0)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (4.5.0)\n",
      "Requirement already satisfied: einops in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.8.2)\n",
      "Requirement already satisfied: fastapi in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.129.0)\n",
      "Requirement already satisfied: flashinfer_python==0.6.1 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.6.1)\n",
      "Requirement already satisfied: flashinfer_cubin==0.6.1 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.6.1)\n",
      "Requirement already satisfied: gguf in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.17.1)\n",
      "Requirement already satisfied: hf_transfer in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.1.9)\n",
      "Requirement already satisfied: interegular in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.3.3)\n",
      "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.7.30)\n",
      "Requirement already satisfied: modelscope in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.34.0)\n",
      "Requirement already satisfied: msgspec in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.20.0)\n",
      "Requirement already satisfied: ninja in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.13.0)\n",
      "Requirement already satisfied: nvidia-cutlass-dsl>=4.3.4 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (4.3.5)\n",
      "Requirement already satisfied: nvidia-ml-py in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (13.590.48)\n",
      "Requirement already satisfied: openai-harmony==0.0.4 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.0.4)\n",
      "Requirement already satisfied: openai==2.6.1 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (2.6.1)\n",
      "Requirement already satisfied: orjson in /opt/tljh/user/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (3.11.6)\n",
      "Requirement already satisfied: outlines==0.1.11 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.1.11)\n",
      "Requirement already satisfied: partial_json_parser in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.2.1.1.post7)\n",
      "Requirement already satisfied: pillow in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (12.0.0)\n",
      "Requirement already satisfied: prometheus-client>=0.20.0 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.20.0)\n",
      "Requirement already satisfied: py-spy in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.4.1)\n",
      "Requirement already satisfied: pybase64 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.4.3)\n",
      "Requirement already satisfied: pydantic in /opt/tljh/user/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (2.12.5)\n",
      "Requirement already satisfied: python-multipart in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.0.22)\n",
      "Requirement already satisfied: pyzmq>=25.1.2 in /opt/tljh/user/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (27.1.0)\n",
      "Requirement already satisfied: quack-kernels==0.2.4 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.2.4)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.17.0)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.2.1)\n",
      "Requirement already satisfied: setproctitle in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.3.7)\n",
      "Requirement already satisfied: sgl-kernel==0.3.21 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.3.21)\n",
      "Requirement already satisfied: soundfile==0.13.1 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.13.1)\n",
      "Requirement already satisfied: tiktoken in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.12.0)\n",
      "Requirement already satisfied: timm==1.0.16 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.0.16)\n",
      "Requirement already satisfied: torch_memory_saver==0.0.9 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.0.9)\n",
      "Requirement already satisfied: torchao==0.9.0 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.9.0)\n",
      "Requirement already satisfied: torchaudio==2.9.1 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (2.9.1)\n",
      "Requirement already satisfied: torchcodec==0.8.0 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.8.0)\n",
      "Requirement already satisfied: torchvision in /opt/tljh/user/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.24.1+cu130)\n",
      "Requirement already satisfied: uvicorn in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.40.0)\n",
      "Requirement already satisfied: uvloop in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.22.1)\n",
      "Requirement already satisfied: xgrammar==0.1.27 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (0.1.27)\n",
      "Requirement already satisfied: grpcio==1.75.1 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.75.1)\n",
      "Requirement already satisfied: grpcio-tools==1.75.1 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.75.1)\n",
      "Requirement already satisfied: grpcio-reflection==1.75.1 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.75.1)\n",
      "Requirement already satisfied: grpcio-health-checking==1.75.1 in ./.local/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (1.75.1)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in ./.local/lib/python3.12/site-packages (from blobfile==3.0.0->sglang>=0.5.8->sglang[all]>=0.5.8) (3.23.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /opt/tljh/user/lib/python3.12/site-packages (from blobfile==3.0.0->sglang>=0.5.8->sglang[all]>=0.5.8) (2.6.3)\n",
      "Requirement already satisfied: lxml>=4.9 in ./.local/lib/python3.12/site-packages (from blobfile==3.0.0->sglang>=0.5.8->sglang[all]>=0.5.8) (6.0.2)\n",
      "Requirement already satisfied: cuda-bindings~=12.9.0 in ./.local/lib/python3.12/site-packages (from cuda-python==12.9->sglang>=0.5.8->sglang[all]>=0.5.8) (12.9.5)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.12/site-packages (from flashinfer_python==0.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (8.1.8)\n",
      "Requirement already satisfied: nvidia-cudnn-frontend>=1.13.0 in ./.local/lib/python3.12/site-packages (from flashinfer_python==0.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (1.18.0)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.12/site-packages (from flashinfer_python==0.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (0.9.0)\n",
      "Requirement already satisfied: protobuf<7.0.0,>=6.31.1 in ./.local/lib/python3.12/site-packages (from grpcio-health-checking==1.75.1->sglang>=0.5.8->sglang[all]>=0.5.8) (6.33.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/tljh/user/lib/python3.12/site-packages (from openai==2.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/tljh/user/lib/python3.12/site-packages (from openai==2.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/tljh/user/lib/python3.12/site-packages (from openai==2.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./.local/lib/python3.12/site-packages (from openai==2.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (0.13.0)\n",
      "Requirement already satisfied: sniffio in ./.local/lib/python3.12/site-packages (from openai==2.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (1.3.1)\n",
      "Requirement already satisfied: lark in /opt/tljh/user/lib/python3.12/site-packages (from outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (1.3.1)\n",
      "Requirement already satisfied: nest_asyncio in /opt/tljh/user/lib/python3.12/site-packages (from outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/tljh/user/lib/python3.12/site-packages (from outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (3.1.2)\n",
      "Requirement already satisfied: diskcache in ./.local/lib/python3.12/site-packages (from outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (5.6.3)\n",
      "Requirement already satisfied: referencing in /opt/tljh/user/lib/python3.12/site-packages (from outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (0.37.0)\n",
      "Requirement already satisfied: jsonschema in /opt/tljh/user/lib/python3.12/site-packages (from outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (4.26.0)\n",
      "Requirement already satisfied: pycountry in ./.local/lib/python3.12/site-packages (from outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in ./.local/lib/python3.12/site-packages (from outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (20260208)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in ./.local/lib/python3.12/site-packages (from outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (0.1.26)\n",
      "Requirement already satisfied: torch-c-dlpack-ext in ./.local/lib/python3.12/site-packages (from quack-kernels==0.2.4->sglang>=0.5.8->sglang[all]>=0.5.8) (0.1.5)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/tljh/user/lib/python3.12/site-packages (from soundfile==0.13.1->sglang>=0.5.8->sglang[all]>=0.5.8) (2.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/tljh/user/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai==2.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (3.11)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in ./.local/lib/python3.12/site-packages (from cuda-bindings~=12.9.0->cuda-python==12.9->sglang>=0.5.8->sglang[all]>=0.5.8) (1.3.4)\n",
      "Requirement already satisfied: certifi in /opt/tljh/user/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==2.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/tljh/user/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==2.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/tljh/user/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==2.6.1->sglang>=0.5.8->sglang[all]>=0.5.8) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic->sglang>=0.5.8->sglang[all]>=0.5.8) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic->sglang>=0.5.8->sglang[all]>=0.5.8) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic->sglang>=0.5.8->sglang[all]>=0.5.8) (0.4.2)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in ./.local/lib/python3.12/site-packages (from anthropic>=0.20.0->sglang>=0.5.8->sglang[all]>=0.5.8) (0.17.0)\n",
      "Requirement already satisfied: pycparser in /opt/tljh/user/lib/python3.12/site-packages (from cffi>=1.0->soundfile==0.13.1->sglang>=0.5.8->sglang[all]>=0.5.8) (3.0)\n",
      "\u001b[33mWARNING: sglang 0.5.8.post1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp->sglang>=0.5.8->sglang[all]>=0.5.8) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp->sglang>=0.5.8->sglang[all]>=0.5.8) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp->sglang>=0.5.8->sglang[all]>=0.5.8) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp->sglang>=0.5.8->sglang[all]>=0.5.8) (6.7.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp->sglang>=0.5.8->sglang[all]>=0.5.8) (1.22.0)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /opt/tljh/user/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp->sglang>=0.5.8->sglang[all]>=0.5.8) (0.4.1)\n",
      "Requirement already satisfied: pyproject_hooks in ./.local/lib/python3.12/site-packages (from build->sglang>=0.5.8->sglang[all]>=0.5.8) (1.2.0)\n",
      "Requirement already satisfied: loguru in ./.local/lib/python3.12/site-packages (from compressed-tensors->sglang>=0.5.8->sglang[all]>=0.5.8) (0.7.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.12/site-packages (from datasets->sglang>=0.5.8->sglang[all]>=0.5.8) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.12/site-packages (from datasets->sglang>=0.5.8->sglang[all]>=0.5.8) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.12/site-packages (from datasets->sglang>=0.5.8->sglang[all]>=0.5.8) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/tljh/user/lib/python3.12/site-packages (from datasets->sglang>=0.5.8->sglang[all]>=0.5.8) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.local/lib/python3.12/site-packages (from datasets->sglang>=0.5.8->sglang[all]>=0.5.8) (0.70.18)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: starlette<1.0.0,>=0.40.0 in ./.local/lib/python3.12/site-packages (from fastapi->sglang>=0.5.8->sglang[all]>=0.5.8) (0.52.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in ./.local/lib/python3.12/site-packages (from fastapi->sglang>=0.5.8->sglang[all]>=0.5.8) (0.0.4)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /opt/tljh/user/lib/python3.12/site-packages (from IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /opt/tljh/user/lib/python3.12/site-packages (from IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /opt/tljh/user/lib/python3.12/site-packages (from IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /opt/tljh/user/lib/python3.12/site-packages (from IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/tljh/user/lib/python3.12/site-packages (from IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/tljh/user/lib/python3.12/site-packages (from IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /opt/tljh/user/lib/python3.12/site-packages (from IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /opt/tljh/user/lib/python3.12/site-packages (from IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/tljh/user/lib/python3.12/site-packages (from IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /opt/tljh/user/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (0.5.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/tljh/user/lib/python3.12/site-packages (from jedi>=0.18.1->IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/tljh/user/lib/python3.12/site-packages (from pexpect>4.3->IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/tljh/user/lib/python3.12/site-packages (from stack_data>=0.6.0->IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/tljh/user/lib/python3.12/site-packages (from stack_data>=0.6.0->IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /opt/tljh/user/lib/python3.12/site-packages (from stack_data>=0.6.0->IPython->sglang>=0.5.8->sglang[all]>=0.5.8) (0.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/tljh/user/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (2025.9.1)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /opt/tljh/user/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->sglang>=0.5.8->sglang[all]>=0.5.8) (0.30.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->datasets->sglang>=0.5.8->sglang[all]>=0.5.8) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.12/site-packages (from pandas->datasets->sglang>=0.5.8->sglang[all]>=0.5.8) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->datasets->sglang>=0.5.8->sglang[all]>=0.5.8) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->sglang>=0.5.8->sglang[all]>=0.5.8) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate torch 'sglang[all]>=0.5.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7064a161-14c2-4c93-976e-7c1f92f133f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF token set in notebook: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ðŸ” Hugging Face access token (READ permission)\n",
    "# IMPORTANT:\n",
    "#  - Use a token that has access to gated models (Llama 3.x)\n",
    "#  - Rotate this token if it was ever shared\n",
    "HF_TOKEN = \"....\"\n",
    "\n",
    "# Make it visible to transformers\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "print(\"HF token set in notebook:\", bool(HF_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15c6d4df-555c-41d4-9972-568a0e32e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "def unload_model(model_name: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Remove model(s) from cache and free GPU memory.\n",
    "    - If model_name is given: unload just that model.\n",
    "    - If model_name is None: unload all cached models.\n",
    "    \"\"\"\n",
    "    global MODEL_CACHE\n",
    "\n",
    "    if model_name is None:\n",
    "        keys = list(MODEL_CACHE.keys())\n",
    "    else:\n",
    "        keys = [model_name] if model_name in MODEL_CACHE else []\n",
    "\n",
    "    for name in keys:\n",
    "        try:\n",
    "            tok, mdl = MODEL_CACHE.pop(name)\n",
    "            del tok\n",
    "            del mdl\n",
    "            print(f\"ðŸ§¹ Unloaded model from memory: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to unload {name}: {e}\")\n",
    "\n",
    "    # Force Python & CUDA to release memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2581c954-9c23-4b7f-9333-e57f34463c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, re, hashlib\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# === Configuration ===\n",
    "DATA_DIR = Path(\"data/6GBench/mcq_questions_only\")\n",
    "SUMMARY_MAX_TURNS = 12\n",
    "\n",
    "# Default model (you can override per run / in cell 7)\n",
    "EVAL_MODEL = \"Qwen/Qwen3-Coder-Next-FP8\"\n",
    "\n",
    "# Simple cache so we don't reload weights every call\n",
    "MODEL_CACHE: Dict[str, Tuple[AutoTokenizer, AutoModelForCausalLM]] = {}\n",
    "\n",
    "def get_local_model(model_name: str) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:\n",
    "    \"\"\"\n",
    "    Load a HF model + tokenizer once and cache it.\n",
    "    Uses GPU (A100) via device_map='auto'.\n",
    "    Works for gated models (Llama 3.x) using in-notebook auth.\n",
    "    \"\"\"\n",
    "    if model_name in MODEL_CACHE:\n",
    "        return MODEL_CACHE[model_name]\n",
    "\n",
    "    print(f\"ðŸ”„ Loading model locally: {model_name}\")\n",
    "\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=HF_TOKEN,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=HF_TOKEN,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure pad token exists\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    MODEL_CACHE[model_name] = (tokenizer, model)\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84544f27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99df20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "TASKS: List[Tuple[str, str, str]] = [\n",
    "  (\n",
    "    \"T1\",\n",
    "    \"Intent Feasibility Assessment\",\n",
    "    \"Given a mission and a 6G intent message, determine whether the intent is feasible under current and near-future network, environmental, and policy constraints. Identify the minimal safe adjustments (e.g., speed, route, slice, autonomy level, sensing configuration) needed to satisfy constraints while preserving mission objectives as much as possible.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T2\",\n",
    "    \"Intent Conflict Resolution\",\n",
    "    \"Resolve conflicts between the operator/mission intent and network or safety policy (e.g., airspace, security, energy, SLA). Decide whether to reject, modify, or conditionally approve the intent, and specify concrete policy-aligned adjustments that balance mission goals and compliance.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T3\",\n",
    "    \"Intent Drift Detection\",\n",
    "    \"Detect subtle changes in mission or network intent over time (e.g., updated priorities, new safety requirements, shifted QoS targets) by comparing past and current intents and behavior. Decide whether the drift is benign, problematic, or requires renegotiation or clarification with other agents or controllers.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T4\",\n",
    "    \"Slice Selection Reasoning\",\n",
    "    \"Given mission requirements and current network telemetry, choose between URLLC, eMBB, or a hybrid slice (or slice configuration) with explicit justification. Trade off latency, reliability, throughput, and robustness, and explain why alternative slices are less appropriate in the given context.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T5\",\n",
    "    \"Slice Switching Decision\",\n",
    "    \"Decide whether to switch, maintain, or augment the current network slice when performance degrades, considering stability, hysteresis, mission criticality, and switching overheads. Prefer decisions that avoid unnecessary oscillations while still preventing SLA violation or safety risk.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T6\",\n",
    "    \"Slice Fairness vs Safety\",\n",
    "    \"Resolve contention for slice resources among multiple agents or swarm members when their demands conflict. Balance fairness, priority levels, and safety margins, possibly degrading some agents more than others, while ensuring global mission safety and compliance with policies.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T7\",\n",
    "    \"Compute Placement Decision\",\n",
    "    \"Choose where to execute AI inference or other compute tasks (onboard, edge, peer, or cloud) under latency, bandwidth, energy, model quality, and trust constraints. Justify placement by considering dynamic network conditions, SLA requirements, and potential failure modes of each location.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T8\",\n",
    "    \"Graceful Degradation under Edge Overload\",\n",
    "    \"When edge resources become overloaded or unstable, decide how to gracefully degrade autonomy or service quality before SLAs are violated. Select which functions to simplify, slow down, or disable while preserving safety-critical behavior and mission viability as long as possible.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T9\",\n",
    "    \"Trust-Aware Offloading\",\n",
    "    \"Evaluate whether to offload tasks or data to edge or third-party compute resources based on trust, security, and policy constraints. Decide when to reject offloading, use partial offloading, or require additional safeguards (e.g., encryption, sandboxing) despite potential performance benefits.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T10\",\n",
    "    \"SLA Violation Prediction\",\n",
    "    \"Predict future SLA violations using early network and system signals such as latency trends, jitter, loss, throughput, edge load, and mission dynamics. Distinguish between transient fluctuations and meaningful trends, and indicate when preemptive mitigation is required to avoid imminent violation.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T11\",\n",
    "    \"Preemptive Autonomy Downgrade\",\n",
    "    \"Before any actual failure or SLA violation occurs, decide when and how to safely downgrade autonomy or functionality based on predicted risk. Choose specific behaviors or capabilities to limit, explaining why the downgrade is justified and how it preserves overall mission safety and compliance.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T12\",\n",
    "    \"Conservative Continuation Decision\",\n",
    "    \"Under uncertainty about network, sensing, or environment, decide whether to continue the mission in a conservative mode or pause/abort. Weigh incomplete or noisy evidence, risk to safety and SLA, and mission criticality, preferring nuanced partial continuation when strictly safe and justifiable.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T13\",\n",
    "    \"Swarm-Level Slice Negotiation\",\n",
    "    \"Coordinate slice allocation across multiple agents or swarm members with competing demands and priorities. Decide how to negotiate and partition slice resources over time, potentially reallocating or renegotiating as conditions change while maintaining global mission performance and fairness.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T14\",\n",
    "    \"Scheduler Reconfiguration Adaptation\",\n",
    "    \"When the underlying AI or network scheduler is reconfigured, updated, or replaced, maintain decision consistency and mission safety. Detect behavioral changes introduced by the new scheduler and adapt policies or intents so that overall system behavior remains coherent and policy-compliant.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T15\",\n",
    "    \"Decision Consistency under Replanning\",\n",
    "    \"Ensure that decisions across multiple planning cycles or turns remain logically consistent with prior commitments, unless new evidence necessitates a change. Avoid contradictory or oscillatory decisions, and when changes are required, justify them with explicit reference to updated context or constraints.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T16\",\n",
    "    \"Network-Exposed Compute Marketplace\",\n",
    "    \"Decide whether, when, and how to expose operator edge/cloud compute resources as a marketplace to third parties under current load, SLAs, and policies. Determine pricing, admission, and allocation strategies that protect critical network services while extracting value from idle capacity.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T17\",\n",
    "    \"Network-Knowledge RAG Augmentation\",\n",
    "    \"Decide what network telemetry, logs, and knowledge to expose to Retrieval-Augmented Generation systems to enhance agent reasoning, under privacy, security, and latency constraints. Balance informativeness against overhead and policy limits, selecting only the most relevant and safe signals.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T18\",\n",
    "    \"AI Agent Identity & Onboarding\",\n",
    "    \"Authorize, authenticate, and register AI agents (device- or network-hosted) and decide how they are represented in identity and access control systems. Define identity mapping, credentials, and onboarding flows that respect policy, security, and interoperability requirements over the agent lifecycle.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T19\",\n",
    "    \"AI Agent Interoperability & Federation\",\n",
    "    \"Resolve compatibility and data-sharing decisions when multiple AI agents from different domains, operators, or networks must collaborate. Decide protocols, translation layers, and data access policies that enable coordination while respecting trust boundaries, privacy, and regulatory constraints.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T20\",\n",
    "    \"Agent-to-Agent Communication Management\",\n",
    "    \"Decide routing, QoS, and security policies for horizontal traffic between AI agents, whether direct or via network relays. Prioritize flows, select paths or slices, and enforce encryption or isolation as needed to meet latency, reliability, and security goals under dynamic network conditions.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T21\",\n",
    "    \"Device-Network Task Offload Arbitration\",\n",
    "    \"Choose whether and how to offload AI or compute tasks from a device to edge, peer, or cloud resources given latency, energy, model capability, and trust constraints. Consider partial offloading, model selection, and fallback strategies, ensuring that decisions remain robust to network fluctuations.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T22\",\n",
    "    \"Federated / Collaborative Learning Orchestration\",\n",
    "    \"Decide when and how to schedule federated or collaborative training and model updates across devices and edge nodes. Respect privacy, regulatory constraints, bandwidth limits, and device heterogeneity, and choose update frequencies and participant sets that balance model quality and resource usage.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T23\",\n",
    "    \"Network-Assisted Digital Twin Control\",\n",
    "    \"Determine how the network should provide sensing, telemetry, and control channels to maintain accurate and actionable real-time digital twins. Decide update rates, data fidelity, and control loop configurations that keep the twin synchronized without overloading the network or violating SLAs.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T24\",\n",
    "    \"Sensing-Enhanced Decisioning (ISAC)\",\n",
    "    \"Choose which sensing streams (e.g., radar, RF, vision, telemetry) and fusion strategies the network should deliver to agents for time-sensitive perception and decision tasks. Trade off sensing accuracy, bandwidth, latency, and robustness, and adapt the sensing configuration as conditions evolve.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T25\",\n",
    "    \"AI-Agent-based Disaster / Public-Safety Coordination\",\n",
    "    \"Coordinate multiple AI agents and UAVs during disaster or public-safety scenarios, deciding slice allocation, sensing priorities, and escalation paths. Balance competing mission goals such as search and rescue, damage assessment, and communication support under extreme and uncertain conditions.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T26\",\n",
    "    \"Trust-Aware Third-Party Agent Exposure\",\n",
    "    \"Decide what level of data, APIs, and compute resources to expose to third-party agents based on trust scores, regulation, and user consent. Enforce differentiated access and isolation policies, and adapt exposure in response to observed behavior, anomalies, or changing regulatory constraints.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T27\",\n",
    "    \"Agent Lifecycle & Management\",\n",
    "    \"Decide lifecycle operations for agents, including instantiation, scaling, migration, upgrade, and retirement, under operator policy and SLA constraints. Coordinate these operations with network load, security posture, and mission demands to avoid service disruption or policy violations.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T28\",\n",
    "    \"6G Model Training-as-a-Service Decision\",\n",
    "    \"Decide when to accept or reject customer requests for network-facilitated model training (e.g., LLM fine-tuning) given resource availability, privacy requirements, and QoS impact. Determine appropriate training configurations, isolation levels, and scheduling so that core network services remain protected.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T29\",\n",
    "    \"Immersive/AR Resource Prioritization\",\n",
    "    \"Allocate slices and edge resources to multi-modal immersive or XR/AR sessions, balancing throughput, latency, stability, and fairness across users and applications. Handle contention by prioritizing critical interactions and gracefully degrading less critical modalities when resources are constrained.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T30\",\n",
    "    \"Network Security Detection & Response Automation\",\n",
    "    \"When AI-driven monitoring flags potential attacks or anomalies, decide automated detection, isolation, mitigation, and recovery actions in the network. Balance swift containment with false-positive risk, and choose responses that preserve critical services and safety while minimizing collateral impact.\"\n",
    "  ),\n",
    "]\n",
    "\n",
    "TASK_MAP: Dict[str, Tuple[str, str]] = {tid: (name, desc) for tid, name, desc in TASKS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33b8a55c-9725-4ce9-ad4e-daaefe759142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks for which we compute stochastic pass@k\n",
    "PASSK_TASKS = {\"T2\", \"T9\", \"T12\", \"T19\", \"T20\", \"T26\", \"T30\"}\n",
    "\n",
    "# Values of k to evaluate\n",
    "PASSK_KS = [3, 5]\n",
    "\n",
    "# Temperature used for stochastic evaluation\n",
    "PASSK_TEMPERATURE = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d31d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "@dataclass\n",
    "class MCQQuestion:\n",
    "    task_id: str\n",
    "    task_name: str\n",
    "    source_turn: int\n",
    "    question: str\n",
    "    options: Dict[str, str]\n",
    "    correct: str\n",
    "    reason: str\n",
    "    rationale_tag: str\n",
    "    difficulty: str\n",
    "\n",
    "def get_turns(ep: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    return ep.get('dialogue', [])\n",
    "\n",
    "def extract_min_context(ep: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    init_state = ep.get('initial_state', {})\n",
    "    env = init_state.get('env') or {}\n",
    "    airspace = init_state.get('airspace') or {}\n",
    "    uav = init_state.get('uav') or {}\n",
    "    policy = init_state.get('policy') or {}\n",
    "    sensors = (uav.get('sensors') or {})\n",
    "    return {\n",
    "        'env': env,\n",
    "        'airspace': {'alt_bounds': airspace.get('alt_bounds'), 'geofence': airspace.get('geofence')},\n",
    "        'uav': {'pose': uav.get('pose'), 'speed_mps': uav.get('speed_mps'), 'battery_pct': (uav.get('energy') or {}).get('battery_pct'), 'sensors': sensors, 'payloads': uav.get('payloads', [])},\n",
    "        'policy': policy,\n",
    "        'success': ep.get('success'),\n",
    "    }\n",
    "\n",
    "def summarize_episode_for_prompt(ep: Dict[str, Any], max_turns: int = 12) -> str:\n",
    "    ctx = extract_min_context(ep)\n",
    "    parts: List[str] = []\n",
    "    parts.append('Initial context:')\n",
    "    parts.append(f\"- env: {ctx['env']}\")\n",
    "    parts.append(f\"- airspace: {ctx['airspace']}\")\n",
    "    parts.append(f\"- uav: {ctx['uav']}\")\n",
    "    parts.append(f\"- policy: {ctx['policy']}\")\n",
    "    parts.append(f\"- success: {ctx['success']}\")\n",
    "    parts.append('')\n",
    "    parts.append('Dialogue trace (truncated):')\n",
    "    for t in get_turns(ep)[:max_turns]:\n",
    "        turn_no = t.get('turn')\n",
    "        speaker = t.get('speaker')\n",
    "        intent = t.get('intent')\n",
    "        acts = t.get('actions', [])\n",
    "        act_summaries = []\n",
    "        for a in acts:\n",
    "            if a.get('type') == 'mcp':\n",
    "                arg_keys = list((a.get('args') or {}).keys())\n",
    "                act_summaries.append(f\"mcp:{a.get('name')} args_keys={arg_keys}\")\n",
    "            elif a.get('type') == 'a2a':\n",
    "                act_summaries.append(f\"a2a:{a.get('task')} to={a.get('to')}\")\n",
    "        obs = t.get('obs', [])\n",
    "        obs_summaries = []\n",
    "        for o in obs[:3]:\n",
    "            if 'tool' in o:\n",
    "                res = o.get('result', {})\n",
    "                status = res.get('status') if isinstance(res, dict) else None\n",
    "                obs_summaries.append(f\"tool:{o.get('tool')} status={status}\")\n",
    "            elif 'task' in o:\n",
    "                obs_summaries.append(f\"a2a_resp:{o.get('task')} status={o.get('status')}\")\n",
    "        net = t.get('net') or {}\n",
    "        net_s = { 'slice': net.get('slice'), 'lat_ms': net.get('lat_ms'), 'jitter_ms': net.get('jitter_ms'), 'loss_pct': net.get('loss_pct'), 'throughput_mbps': net.get('throughput_mbps'), 'edge_load': net.get('edge_load') }\n",
    "        parts.append(f\"- turn={turn_no} speaker={speaker} intent={intent} actions={act_summaries} obs={obs_summaries} net={net_s}\")\n",
    "    return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c8db315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_chat_debug(\n",
    "    model: str,\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.2,\n",
    "    max_tokens: int = 512,  # safer default for max_new_tokens\n",
    "    seed: Optional[int] = None,\n",
    ") -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run a local HF model in chat style and return (completion_text, debug_info).\n",
    "    \"\"\"\n",
    "    tokenizer, lm = get_local_model(model)\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Build a chat prompt\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        chat = [\n",
    "            {\"role\": m.get(\"role\", \"user\"), \"content\": m.get(\"content\", \"\")}\n",
    "            for m in messages\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    else:\n",
    "        # Fallback: simple system + user concatenation\n",
    "        system_parts, user_parts = [], []\n",
    "        for m in messages:\n",
    "            if m[\"role\"] == \"system\":\n",
    "                system_parts.append(m[\"content\"])\n",
    "            else:\n",
    "                user_parts.append(m[\"content\"])\n",
    "        system_text = \"\\n\\n\".join(system_parts).strip()\n",
    "        user_text = \"\\n\\n\".join(user_parts).strip()\n",
    "        prompt = f\"{system_text}\\n\\n{user_text}\" if system_text else user_text\n",
    "\n",
    "    max_ctx = getattr(lm.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "    # First tokenize prompt (truncate if needed)\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_ctx,  # let the tokenizer handle truncation\n",
    "    ).to(lm.device)\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    # Ensure we don't exceed context window\n",
    "    max_new_tokens = min(max_tokens, max_ctx - input_len)\n",
    "    if max_new_tokens <= 0:\n",
    "        # Extreme case: prompt ~max_ctx already\n",
    "        max_new_tokens = 1  # at least try to generate something\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    if pad_id is None:\n",
    "        pad_id = tokenizer.eos_token_id\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": temperature > 0.0,\n",
    "        \"temperature\": temperature,\n",
    "        \"pad_token_id\": pad_id,\n",
    "    }\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    if eos_id is not None:\n",
    "        gen_kwargs[\"eos_token_id\"] = eos_id\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = lm.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    # Slice off the prompt like your Qwen3-Coder-Next example\n",
    "    gen_ids = output_ids[0, input_len:]\n",
    "    completion = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "    debug_info = {\n",
    "        \"prompt\": prompt,\n",
    "        \"input_ids\": inputs[\"input_ids\"][0].tolist(),\n",
    "        \"output_ids\": output_ids[0].tolist(),\n",
    "        \"max_ctx\": max_ctx,\n",
    "        \"used_max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "    return completion.strip(), debug_info\n",
    "\n",
    "\n",
    "def build_eval_messages(\n",
    "    task_id: str,\n",
    "    episode_summary: str,\n",
    "    question: Dict[str, Any],\n",
    ") -> List[Dict[str, str]]:\n",
    "    task_name, task_def = TASK_MAP[task_id]\n",
    "    q_text = question['question']\n",
    "    options = question['options']\n",
    "\n",
    "    user_content: List[str] = []\n",
    "    user_content.append('TARGET TASK')\n",
    "    user_content.append(f'TASK_ID: {task_id}')\n",
    "    user_content.append(f'TASK_NAME: {task_name}')\n",
    "    user_content.append(f'DEFINITION: {task_def}')\n",
    "    user_content.append('')\n",
    "    user_content.append('EPISODE SUMMARY')\n",
    "    user_content.append(episode_summary)\n",
    "    user_content.append('')\n",
    "    user_content.append('QUESTION')\n",
    "    user_content.append(q_text)\n",
    "    user_content.append('')\n",
    "    user_content.append('OPTIONS')\n",
    "    for k in ['A', 'B', 'C', 'D']:\n",
    "        user_content.append(f\"{k}: {options[k]}\")\n",
    "    user_content.append('')\n",
    "    user_content.append(\n",
    "        'Respond ONLY with valid JSON of the form {\"answer\": \"A/B/C/D\"} where answer is one of \"A\", \"B\", \"C\", or \"D\".'\n",
    "    )\n",
    "\n",
    "    system_msg = (\n",
    "        'You are an expert 6G network AI agent evaluator. '\n",
    "        'You will answer a multiple-choice question (A/B/C/D) about a UAV mission episode. '\n",
    "        'Use the episode context and the target task definition to choose the best answer. '\n",
    "        'You MUST respond with a single JSON object of the form {\"answer\": \"A\" or \"B\" or \"C\" or \"D\"}.'\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {'role': 'system', 'content': system_msg},\n",
    "        {'role': 'user', 'content': '\\n'.join(user_content)},\n",
    "    ]\n",
    "\n",
    "\n",
    "def parse_mcq_answer_from_json(raw) -> Optional[str]:\n",
    "    if isinstance(raw, dict):\n",
    "        obj = raw\n",
    "    else:\n",
    "        try:\n",
    "            obj = json.loads(raw)\n",
    "        except Exception:\n",
    "            obj = None\n",
    "    if isinstance(obj, dict):\n",
    "        for key in ['answer', 'choice', 'label', 'option']:\n",
    "            val = obj.get(key)\n",
    "            if isinstance(val, str) and val.strip() in {'A', 'B', 'C', 'D'}:\n",
    "                return val.strip()\n",
    "\n",
    "    text = raw if isinstance(raw, str) else str(raw)\n",
    "    m = re.search(r'\"answer\"\\s*:\\s*\"([ABCD])\"', text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m2 = re.search(r'\\b([ABCD])\\b', text)\n",
    "    if m2:\n",
    "        return m2.group(1)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6a9f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_episode_mcq_pairs(data_dir: Path) -> List[Tuple[str, Dict[str, Any], Dict[str, Any]]]:\n",
    "    episodes: Dict[str, Path] = {}\n",
    "    mcqs: Dict[str, Path] = {}\n",
    "\n",
    "    # Collect all episode and MCQ files\n",
    "    for p in data_dir.glob('*.episode.json'):\n",
    "        base = p.name.replace('.episode.json', '')\n",
    "        episodes[base] = p\n",
    "\n",
    "    for p in data_dir.glob('*.mcq.json'):\n",
    "        base = p.name.replace('.mcq.json', '')\n",
    "        mcqs[base] = p\n",
    "\n",
    "    # Only keep keys that have both episode and mcq\n",
    "    common_keys = sorted(set(episodes.keys()) & set(mcqs.keys()))\n",
    "    pairs: List[Tuple[str, Dict[str, Any], Dict[str, Any]]] = []\n",
    "\n",
    "    for key in common_keys:\n",
    "        with episodes[key].open('r', encoding='utf-8') as f_ep:\n",
    "            ep = json.load(f_ep)\n",
    "        with mcqs[key].open('r', encoding='utf-8') as f_q:\n",
    "            mcq = json.load(f_q)\n",
    "        episode_id = mcq.get('episode_id', key)\n",
    "        pairs.append((episode_id, ep, mcq))\n",
    "\n",
    "    print(f'Found {len(pairs)} episode/MCQ pairs in {data_dir}')\n",
    "    return pairs\n",
    "\n",
    "def eval_model_on_dir(\n",
    "    data_dir: Path,\n",
    "    model: str,\n",
    "    max_pairs: Optional[int] = None,\n",
    "    seed_base: Optional[int] = 42,\n",
    ") -> Dict[str, Any]:\n",
    "    # KEEP using episode_mcq_pairs (unchanged)\n",
    "    pairs = load_episode_mcq_pairs(data_dir)\n",
    "    if max_pairs is not None:\n",
    "        pairs = pairs[:max_pairs]\n",
    "    # --- count total questions across all episodes ---\n",
    "    total_questions_all = sum(\n",
    "        len(mcq.get(\"questions\", [])) for _, _, mcq in pairs\n",
    "    )\n",
    "\n",
    "    avg_q_per_episode = (\n",
    "        total_questions_all / len(pairs) if pairs else 0.0\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Total episodes: {len(pairs)} | \"\n",
    "        f\"Total questions (all episodes): {total_questions_all} | \"\n",
    "        f\"Avg questions/episode: {avg_q_per_episode:.2f}\"\n",
    "    )\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    per_task_stats: Dict[str, Dict[str, int]] = {}\n",
    "    per_question_records: List[Dict[str, Any]] = []\n",
    "    # pass@k stats: {task_id: {k: {\"total\": int, \"hit\": int}}}\n",
    "    per_task_passk_stats: Dict[str, Dict[int, Dict[str, int]]] = {}\n",
    "    # Also track overall pass@k across all selected tasks\n",
    "    overall_passk_stats: Dict[int, Dict[str, int]] = {\n",
    "        k: {\"total\": 0, \"hit\": 0} for k in PASSK_KS\n",
    "    }\n",
    "\n",
    "    # Text progress bar (works well in VS Code)\n",
    "    pbar = tqdm(\n",
    "        total=len(pairs),\n",
    "        desc=\"Eval episodes\",\n",
    "        unit=\"ep\",\n",
    "    )\n",
    "\n",
    "    for episode_id, ep, mcq in pairs:\n",
    "        episode_summary = summarize_episode_for_prompt(ep, max_turns=SUMMARY_MAX_TURNS)\n",
    "        questions = mcq.get('questions', [])\n",
    "\n",
    "        for q in questions:\n",
    "            task_id = q['task_id']\n",
    "            gold = q['correct']\n",
    "\n",
    "            messages = build_eval_messages(task_id, episode_summary, q)\n",
    "\n",
    "            seed = None\n",
    "            if seed_base is not None:\n",
    "                s = f\"{seed_base}:{episode_id}:{task_id}\".encode('utf-8')\n",
    "                seed = int(hashlib.sha256(s).hexdigest()[:8], 16)\n",
    "\n",
    "            raw_text, debug_info = local_chat_debug(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "                max_tokens=10000,  # or whatever you prefer\n",
    "                seed=seed,\n",
    "            )\n",
    "            \n",
    "            # local_chat_debug already returns plain text, no OpenRouter JSON\n",
    "            api_parsed = None\n",
    "            extracted_content = None\n",
    "            \n",
    "            parse_input = raw_text\n",
    "            pred = parse_mcq_answer_from_json(parse_input)\n",
    "\n",
    "            is_correct = int(pred == gold)\n",
    "            total += 1\n",
    "            correct += is_correct\n",
    "\n",
    "            stats = per_task_stats.setdefault(task_id, {'total': 0, 'correct': 0})\n",
    "            stats['total'] += 1\n",
    "            stats['correct'] += is_correct\n",
    "\n",
    "            per_question_records.append({\n",
    "                'episode_id': episode_id,\n",
    "                'task_id': task_id,\n",
    "                'task_name': q.get('task_name'),\n",
    "                'difficulty': q.get('difficulty'),\n",
    "                'pred': pred,\n",
    "                'gold': gold,\n",
    "                'correct_flag': is_correct,\n",
    "                'raw_model_output': raw_text,\n",
    "                'api_parsed_json': api_parsed,      # now always None\n",
    "                'extracted_content': extracted_content,  # now always None\n",
    "                'question': q.get('question'),\n",
    "                'options': q.get('options'),\n",
    "                'source_turn': q.get('source_turn'),\n",
    "            })\n",
    "                        # --- Optional stochastic pass@k evaluation on selected tasks ---\n",
    "            if task_id in PASSK_TASKS:\n",
    "                task_passk = per_task_passk_stats.setdefault(task_id, {})\n",
    "                for k_val in PASSK_KS:\n",
    "                    k_stats = task_passk.setdefault(k_val, {\"total\": 0, \"hit\": 0})\n",
    "                    k_stats[\"total\"] += 1\n",
    "                    overall_passk_stats[k_val][\"total\"] += 1\n",
    "            \n",
    "                    hit = False\n",
    "                    for attempt in range(k_val):\n",
    "                        attempt_seed = None\n",
    "                        if seed_base is not None:\n",
    "                            s_k = f\"{seed_base}:{episode_id}:{task_id}:k{k_val}:a{attempt}\".encode(\"utf-8\")\n",
    "                            attempt_seed = int(hashlib.sha256(s_k).hexdigest()[:8], 16)\n",
    "            \n",
    "                        raw_k, debug_k = local_chat_debug(\n",
    "                            model=model,\n",
    "                            messages=messages,\n",
    "                            temperature=PASSK_TEMPERATURE,\n",
    "                            max_tokens=10000,\n",
    "                            seed=attempt_seed,\n",
    "                        )\n",
    "            \n",
    "                        pred_k = parse_mcq_answer_from_json(raw_k)\n",
    "            \n",
    "                        if pred_k == gold:\n",
    "                            hit = True\n",
    "                            break\n",
    "            \n",
    "                    if hit:\n",
    "                        k_stats[\"hit\"] += 1\n",
    "                        overall_passk_stats[k_val][\"hit\"] += 1\n",
    "            \n",
    "\n",
    "\n",
    "        # === REAL-TIME METRICS ===\n",
    "        incorrect = total - correct\n",
    "        current_acc = correct / total if total else 0.0\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(\n",
    "            q=total,\n",
    "            ok=correct,\n",
    "            wrong=incorrect,\n",
    "            acc=f\"{current_acc:.3f}\",\n",
    "        )\n",
    "\n",
    "        tqdm.write(\n",
    "            f\"[running] episodes={pbar.n}/{pbar.total} | \"\n",
    "            f\"questions={total} | overall_acc={current_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    overall_acc = correct / total if total else 0.0\n",
    "    per_task_acc = {\n",
    "        tid: s['correct'] / s['total'] if s['total'] else 0.0\n",
    "        for tid, s in per_task_stats.items()\n",
    "    }\n",
    "\n",
    "    # --- Aggregate pass@k stats ---\n",
    "    per_task_passk: Dict[str, Dict[int, float]] = {}\n",
    "    for tid, k_stats_dict in per_task_passk_stats.items():\n",
    "        per_task_passk[tid] = {}\n",
    "        for k_val, stats_k in k_stats_dict.items():\n",
    "            if stats_k[\"total\"]:\n",
    "                per_task_passk[tid][k_val] = stats_k[\"hit\"] / stats_k[\"total\"]\n",
    "            else:\n",
    "                per_task_passk[tid][k_val] = 0.0\n",
    "\n",
    "    overall_passk: Dict[int, float] = {}\n",
    "    for k_val, stats_k in overall_passk_stats.items():\n",
    "        if stats_k[\"total\"]:\n",
    "            overall_passk[k_val] = stats_k[\"hit\"] / stats_k[\"total\"]\n",
    "        else:\n",
    "            overall_passk[k_val] = 0.0\n",
    "\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'per_task_accuracy': per_task_acc,\n",
    "        'overall_passk': overall_passk,            # NEW\n",
    "        'per_task_passk': per_task_passk,          # NEW\n",
    "        'total_questions': total,\n",
    "        'records': per_question_records,\n",
    "    }\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dea7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_task_question_counts(data_dir: Path) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Print how many MCQ questions exist per task across all episodes.\n",
    "    Returns a dict {task_id: count}.\n",
    "    \"\"\"\n",
    "    pairs = load_episode_mcq_pairs(data_dir)\n",
    "\n",
    "    task_counts: Dict[str, int] = {}\n",
    "    total_questions = 0\n",
    "\n",
    "    for _, _, mcq in pairs:\n",
    "        for q in mcq.get(\"questions\", []):\n",
    "            task_id = q.get(\"task_id\", \"UNKNOWN\")\n",
    "            task_counts[task_id] = task_counts.get(task_id, 0) + 1\n",
    "            total_questions += 1\n",
    "\n",
    "    print(\"\\nQuestion count per task:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for task_id, count in sorted(task_counts.items()):\n",
    "        task_name, _ = TASK_MAP.get(task_id, (\"?\", \"\"))\n",
    "        print(f\"{task_id:>3} | {task_name:<45} | {count:5d}\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"TOTAL QUESTIONS: {total_questions}\")\n",
    "\n",
    "    return task_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f434e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 488 episode/MCQ pairs in data/6GBench/mcq_questions_only\n",
      "\n",
      "Question count per task:\n",
      "------------------------------------------------------------\n",
      " T1 | Intent Feasibility Assessment                 |   115\n",
      "T10 | SLA Violation Prediction                      |   115\n",
      "T11 | Preemptive Autonomy Downgrade                 |   119\n",
      "T12 | Conservative Continuation Decision            |   107\n",
      "T13 | Swarm-Level Slice Negotiation                 |   128\n",
      "T14 | Scheduler Reconfiguration Adaptation          |   107\n",
      "T15 | Decision Consistency under Replanning         |   129\n",
      "T16 | Network-Exposed Compute Marketplace           |   155\n",
      "T17 | Network-Knowledge RAG Augmentation            |   154\n",
      "T18 | AI Agent Identity & Onboarding                |   157\n",
      "T19 | AI Agent Interoperability & Federation        |   129\n",
      " T2 | Intent Conflict Resolution                    |   113\n",
      "T20 | Agent-to-Agent Communication Management       |   105\n",
      "T21 | Device-Network Task Offload Arbitration       |   132\n",
      "T22 | Federated / Collaborative Learning Orchestration |   144\n",
      "T23 | Network-Assisted Digital Twin Control         |   114\n",
      "T24 | Sensing-Enhanced Decisioning (ISAC)           |   138\n",
      "T25 | AI-Agent-based Disaster / Public-Safety Coordination |   112\n",
      "T26 | Trust-Aware Third-Party Agent Exposure        |   142\n",
      "T27 | Agent Lifecycle & Management                  |   106\n",
      "T28 | 6G Model Training-as-a-Service Decision       |   114\n",
      "T29 | Immersive/AR Resource Prioritization          |   146\n",
      " T3 | Intent Drift Detection                        |   112\n",
      "T30 | Network Security Detection & Response Automation |   145\n",
      " T4 | Slice Selection Reasoning                     |   100\n",
      " T5 | Slice Switching Decision                      |   118\n",
      " T6 | Slice Fairness vs Safety                      |   104\n",
      " T7 | Compute Placement Decision                    |   117\n",
      " T8 | Graceful Degradation under Edge Overload      |   130\n",
      " T9 | Trust-Aware Offloading                        |   115\n",
      "------------------------------------------------------------\n",
      "TOTAL QUESTIONS: 3722\n"
     ]
    }
   ],
   "source": [
    "task_counts = print_task_question_counts(DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "622f9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 488 episode/MCQ pairs in data/6GBench/mcq_questions_only\n",
      "--- Diagnostic: prompt length ---\n",
      "5681 chars\n",
      "ðŸ”„ Loading model locally: Qwen/Qwen3-Coder-Next-FP8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c60dd2972d4646a0b21c97c4bd60ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1dd048cbcd4d3892ed5be08c4ca93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e8cfb29a2c4fe989911c946babecce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01579d9980374cbaac7f5067c7b0f811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4435bd65b091492a9258862ef8504fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5af0030f6b47e79c30b460a46c6b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23659f4a26874f05abefb35aed47d164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dcd32967424ff6ac7efca66ac467d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada3bf4d326c4ebfa3eaa9767814adc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d93ca41d3da49cda08912734a0f52f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83652324fbcb4932a843cd29454018b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b14938b1fef4839a71fb9b8a637b047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7046fbdec74e7ba378d3df517591c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf8ef7282594f8ebb3e80f80e8069cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e6507611ff47f68775125f017761df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368ba9fb886a490aa4086de9317dc171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00040.safetensors:   0%|          | 0.00/2.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2f5aeacc7d41b28ff2a6407a4a44c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957be9d96f02400188e7ff5614aa4b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8a7c4d51144a70a0f8eed21700d7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476e6e25049c4a29bcfe12bc1799bd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bb29044b274a60aabd74cb6ba44211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846268d8060046b5a46d1607554bad6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f69c53582449dea69d0826c402e435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bb9dbed7154ae7a935c4cba59c443e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1351154ddc424643889d332684c9fe4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d9b2e3e36246a291bf266882de9780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1998371cf19464eb7e8e5d7d9ddcf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c08d1a75b5420ebce6bcc1002037d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb019b2476a444e9f46a67b2d9f8c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721b87fa53d744aca73254534dac7e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4721c6d762d4a3688bb5a518f8d08e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce290a819be4cfabb7a7262cc1387f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b0af07618740eab0f6db170a015a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a155ce9937b141998847bc5e99446063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3112103ddec441569a03ed893400ae54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9686b1478c4fa481a7ad10e9854a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412386751010450e86a23cf2e91bf681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8187715b08014979aee14cbdd5c90305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23dff2faa4b846ab91f0ca280678cb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00031-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecbbff1127747eabbaae520ef9cdf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00032-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34ef39b12d149068f4e445f484710bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00033-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142e4d3f7244429db0a57623b60b7736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00034-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2e04ce02cf4ccca628c072e296d424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00035-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb8f4675ca44a08a03f289a1b07e7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00036-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e73a3272cf14f6b86f55cb7d5dd844a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00037-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56d3d5438cf43008e5113c6ad396375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00038-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdd9038155341aca1f78133dc05a387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00039-of-00040.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce348a78f39e4ecdb5d779e62308acac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00040-of-00040.safetensors:   0%|          | 0.00/1.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of the required library is not installed. Falling back to torch implementation. To install follow https://github.com/fla-org/flash-linear-attention#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa8de5f7b144b91a03b9454402cbc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218d4c4c891448b692d5c7f3ea7a0e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Completion (first 2000 chars) ---\n",
      "{\"answer\": \"C\"}\n",
      "\n",
      "--- Parsed answer ---\n",
      "pred = C\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC: run one local request and inspect output\n",
    "pairs = load_episode_mcq_pairs(DATA_DIR)\n",
    "if len(pairs) == 0:\n",
    "    raise RuntimeError(f'No episode/mcq pairs found in {DATA_DIR}')\n",
    "\n",
    "episode_id, ep, mcq = pairs[0]\n",
    "q = mcq['questions'][0]\n",
    "task_id = q['task_id']\n",
    "\n",
    "messages = build_eval_messages(\n",
    "    task_id,\n",
    "    summarize_episode_for_prompt(ep, SUMMARY_MAX_TURNS),\n",
    "    q\n",
    ")\n",
    "\n",
    "print('--- Diagnostic: prompt length ---')\n",
    "print(sum(len(m[\"content\"]) for m in messages), \"chars\")\n",
    "\n",
    "raw_text, debug_info = local_chat_debug(\n",
    "    model=EVAL_MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    "    max_tokens=10000,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print('\\n--- Completion (first 2000 chars) ---')\n",
    "print(raw_text[:2000])\n",
    "\n",
    "print('\\n--- Parsed answer ---')\n",
    "print(\"pred =\", parse_mcq_answer_from_json(raw_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Eval episodes:   0%|          | 0/488 [00:00<?, ?ep/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# List of models to evaluate\n",
    "EVAL_MODELS = [\n",
    "    \"Qwen/Qwen3-Coder-Next-FP8\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    # \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "]\n",
    "\n",
    "TXT_LOG_PATH = Path(\n",
    "    \"data/6GBench/\"\n",
    "    \"6gbench_mcq_eval_cell_output_Qwen3_Coder_Next_FP8.txt\"\n",
    ")\n",
    "\n",
    "TIME_LOG_PATH = Path(\n",
    "    \"data/6GBench/\"\n",
    "    \"6gbench_mcq_eval_timing_Qwen3_Coder_Next_FP8.txt\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Start timer before running anything ----\n",
    "start_time = time.time()\n",
    "\n",
    "# Open log file once, append mode\n",
    "with TXT_LOG_PATH.open(\"a\", encoding=\"utf-8\") as log_file:\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = log_file  # redirect ALL prints to file\n",
    "\n",
    "    try:\n",
    "        for EVAL_MODEL in tqdm(EVAL_MODELS, desc=\"Evaluating models\"):\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"Evaluating model: {EVAL_MODEL}\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "            # Run evaluation (all its prints are captured)\n",
    "            results = eval_model_on_dir(\n",
    "                DATA_DIR,\n",
    "                EVAL_MODEL,\n",
    "                max_pairs=None,\n",
    "                seed_base=42\n",
    "            )\n",
    "\n",
    "            print(f\"Model: {results['model']}\")\n",
    "            print(f\"Total questions: {results['total_questions']}\")\n",
    "            print(f\"Overall accuracy: {results['overall_accuracy']:.3f}\")\n",
    "\n",
    "            print(\"\\nPer-task accuracy:\")\n",
    "            for tid, acc in sorted(results['per_task_accuracy'].items()):\n",
    "                tname, _ = TASK_MAP.get(tid, ('?', ''))\n",
    "                print(f\"  {tid} ({tname}): {acc:.3f}\")\n",
    "            # --- Print overall pass@k for selected tasks ---\n",
    "            if \"overall_passk\" in results:\n",
    "                print(\"\\nOverall pass@k on selected tasks:\")\n",
    "                for k_val, v in sorted(results[\"overall_passk\"].items()):\n",
    "                    print(f\"  pass@{k_val}: {v:.3f}\")\n",
    "\n",
    "            # --- Print per-task pass@k for selected tasks ---\n",
    "            if \"per_task_passk\" in results:\n",
    "                print(\"\\nPer-task pass@k (selected tasks only):\")\n",
    "                for tid in sorted(results[\"per_task_passk\"].keys()):\n",
    "                    tname, _ = TASK_MAP.get(tid, ('?', ''))\n",
    "                    line = f\"  {tid} ({tname}):\"\n",
    "                    for k_val in sorted(results[\"per_task_passk\"][tid].keys()):\n",
    "                        v = results[\"per_task_passk\"][tid][k_val]\n",
    "                        line += f\"  pass@{k_val}={v:.3f}\"\n",
    "                    print(line)\n",
    "\n",
    "            recs = results['records']\n",
    "            none_preds = sum(1 for r in recs if r['pred'] is None)\n",
    "            print(\n",
    "                f\"\\nParsed predictions for {len(recs) - none_preds}/{len(recs)} questions.\"\n",
    "            )\n",
    "\n",
    "            # ----- Save global results (ONE file per model) -----\n",
    "            safe_model_name = EVAL_MODEL.replace(\"/\", \"_\")\n",
    "            out_path = Path(f\"6gbench_mcq_eval_results_{safe_model_name}_debug.json\")\n",
    "\n",
    "            with out_path.open('w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            print(f'Saved detailed global results to {out_path.resolve()}')\n",
    "\n",
    "            # ----- Save per-task results (still per model) -----\n",
    "            per_task_dir = Path(\n",
    "                \"data/6GBench/mcq_eval_per_task_debug\"\n",
    "            )\n",
    "            per_task_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            per_task_records = {}\n",
    "            for r in results['records']:\n",
    "                per_task_records.setdefault(r['task_id'], []).append(r)\n",
    "\n",
    "            for tid, task_recs in per_task_records.items():\n",
    "                tname, _ = TASK_MAP.get(tid, ('?', ''))\n",
    "                safe_name = tname.replace(' ', '_').replace('/', '_')\n",
    "                fname = per_task_dir / f\"{safe_model_name}_{tid}_{safe_name}.json\"\n",
    "\n",
    "                with fname.open('w', encoding='utf-8') as f:\n",
    "                    json.dump(task_recs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "                print(f'Saved {len(task_recs)} records for {tid} ({tname}) to {fname}')\n",
    "\n",
    "    finally:\n",
    "        # ---- Stop timer and write runtime into the .txt log ----\n",
    "        end_time = time.time()\n",
    "        elapsed_seconds = end_time - start_time\n",
    "        elapsed_td = timedelta(seconds=int(elapsed_seconds))\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(f\"Total runtime (seconds): {elapsed_seconds:.2f}\")\n",
    "        print(f\"Total runtime (h:mm:ss): {elapsed_td}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # restore stdout to original\n",
    "        sys.stdout = old_stdout  # always restore stdout\n",
    "\n",
    "print(f\"âœ” Full cell output (including runtime) appended to {TXT_LOG_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8befa7a-9143-47c1-8fda-32d0c693e454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
