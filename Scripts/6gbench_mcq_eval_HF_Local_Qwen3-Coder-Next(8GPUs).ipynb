{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "610d42f2-3242-4372-82e0-5c4ec7dc03f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.12/site-packages (5.1.0)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.12/site-packages (2.5.1+cu121)\n",
      "Collecting vllm>=0.15.0\n",
      "  Downloading vllm-0.15.1-cp38-abi3-manylinux_2_31_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting sglang>=0.5.8 (from sglang[all]>=0.5.8)\n",
      "  Downloading sglang-0.5.8.post1-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in ./.local/lib/python3.12/site-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in ./.local/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/tljh/user/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in ./.local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /opt/tljh/user/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /opt/tljh/user/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/tljh/user/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/tljh/user/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/tljh/user/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: psutil in /opt/tljh/user/lib/python3.12/site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/tljh/user/lib/python3.12/site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.9.86)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting cachetools (from vllm>=0.15.0)\n",
      "  Downloading cachetools-7.0.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting sentencepiece (from vllm>=0.15.0)\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/tljh/user/lib/python3.12/site-packages (from vllm>=0.15.0) (2.32.5)\n",
      "Collecting blake3 (from vllm>=0.15.0)\n",
      "  Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting py-cpuinfo (from vllm>=0.15.0)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: protobuf>=6.33.5 in /opt/tljh/user/lib/python3.12/site-packages (from vllm>=0.15.0) (6.33.5)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm>=0.15.0)\n",
      "  Downloading fastapi-0.129.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: aiohttp>=3.13.3 in /opt/tljh/user/lib/python3.12/site-packages (from vllm>=0.15.0) (3.13.3)\n",
      "Collecting openai>=1.99.1 (from vllm>=0.15.0)\n",
      "  Downloading openai-2.20.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pydantic>=2.12.0 in /opt/tljh/user/lib/python3.12/site-packages (from vllm>=0.15.0) (2.12.5)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /opt/tljh/user/lib/python3.12/site-packages (from vllm>=0.15.0) (0.24.1)\n",
      "Requirement already satisfied: pillow in ./.local/lib/python3.12/site-packages (from vllm>=0.15.0) (12.0.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm>=0.15.0)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm>=0.15.0)\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer==0.11.3 (from vllm>=0.15.0)\n",
      "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<1.4.0,>=1.3.0 (from vllm>=0.15.0)\n",
      "  Downloading llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines_core==0.2.11 (from vllm>=0.15.0)\n",
      "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting diskcache==5.6.3 (from vllm>=0.15.0)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lark==1.2.2 (from vllm>=0.15.0)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.29 (from vllm>=0.15.0)\n",
      "  Downloading xgrammar-0.1.29-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting partial-json-parser (from vllm>=0.15.0)\n",
      "  Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /opt/tljh/user/lib/python3.12/site-packages (from vllm>=0.15.0) (27.1.0)\n",
      "Collecting msgspec (from vllm>=0.15.0)\n",
      "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting gguf>=0.17.0 (from vllm>=0.15.0)\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.8.8 (from mistral_common[image]>=1.8.8->vllm>=0.15.0)\n",
      "  Downloading mistral_common-1.9.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.13.0 (from vllm>=0.15.0)\n",
      "  Downloading opencv_python_headless-4.13.0.92-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/tljh/user/lib/python3.12/site-packages (from vllm>=0.15.0) (1.17.0)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.10.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting einops (from vllm>=0.15.0)\n",
      "  Downloading einops-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.13.0 (from vllm>=0.15.0)\n",
      "  Downloading compressed_tensors-0.13.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.20.0 (from vllm>=0.15.0)\n",
      "  Downloading depyf-0.20.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: cloudpickle in /opt/tljh/user/lib/python3.12/site-packages (from vllm>=0.15.0) (3.1.2)\n",
      "Collecting watchfiles (from vllm>=0.15.0)\n",
      "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-json-logger in /opt/tljh/user/lib/python3.12/site-packages (from vllm>=0.15.0) (4.0.0)\n",
      "Collecting ninja (from vllm>=0.15.0)\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pybase64 (from vllm>=0.15.0)\n",
      "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting cbor2 (from vllm>=0.15.0)\n",
      "  Downloading cbor2-5.8.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting ijson (from vllm>=0.15.0)\n",
      "  Downloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
      "Collecting setproctitle (from vllm>=0.15.0)\n",
      "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
      "Collecting openai-harmony>=0.0.3 (from vllm>=0.15.0)\n",
      "  Downloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting anthropic>=0.71.0 (from vllm>=0.15.0)\n",
      "  Downloading anthropic-0.79.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting model-hosting-container-standards<1.0.0,>=0.1.13 (from vllm>=0.15.0)\n",
      "  Downloading model_hosting_container_standards-0.1.13-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting mcp (from vllm>=0.15.0)\n",
      "  Downloading mcp-1.26.0-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting grpcio (from vllm>=0.15.0)\n",
      "  Downloading grpcio-1.78.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-reflection (from vllm>=0.15.0)\n",
      "  Downloading grpcio_reflection-1.78.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting numba==0.61.2 (from vllm>=0.15.0)\n",
      "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm>=0.15.0)\n",
      "  Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchaudio==2.9.1 (from vllm>=0.15.0)\n",
      "  Downloading torchaudio-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting torchvision==0.24.1 (from vllm>=0.15.0)\n",
      "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting flashinfer-python==0.6.1 (from vllm>=0.15.0)\n",
      "  Downloading flashinfer_python-0.6.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting loguru (from compressed-tensors==0.13.0->vllm>=0.15.0)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting astor (from depyf==0.20.0->vllm>=0.15.0)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dill (from depyf==0.20.0->vllm>=0.15.0)\n",
      "  Downloading dill-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting apache-tvm-ffi!=0.1.8,!=0.1.8.post0,<0.2,>=0.1.6 (from flashinfer-python==0.6.1->vllm>=0.15.0)\n",
      "  Downloading apache_tvm_ffi-0.1.8.post2-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click in /opt/tljh/user/lib/python3.12/site-packages (from flashinfer-python==0.6.1->vllm>=0.15.0) (8.3.1)\n",
      "Collecting nvidia-cudnn-frontend>=1.13.0 (from flashinfer-python==0.6.1->vllm>=0.15.0)\n",
      "  Downloading nvidia_cudnn_frontend-1.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting nvidia-cutlass-dsl>=4.3.4 (from flashinfer-python==0.6.1->vllm>=0.15.0)\n",
      "  Downloading nvidia_cutlass_dsl-4.3.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-ml-py (from flashinfer-python==0.6.1->vllm>=0.15.0)\n",
      "  Downloading nvidia_ml_py-13.590.48-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting tabulate (from flashinfer-python==0.6.1->vllm>=0.15.0)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm>=0.15.0)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm>=0.15.0)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting numpy>=1.17 (from accelerate)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.16.4 (from tokenizers<=0.23.0,>=0.22.0->transformers)\n",
      "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting jmespath (from model-hosting-container-standards<1.0.0,>=0.1.13->vllm>=0.15.0)\n",
      "  Downloading jmespath-1.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting starlette>=0.49.1 (from model-hosting-container-standards<1.0.0,>=0.1.13->vllm>=0.15.0)\n",
      "  Downloading starlette-0.52.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting supervisor>=4.2.0 (from model-hosting-container-standards<1.0.0,>=0.1.13->vllm>=0.15.0)\n",
      "  Downloading supervisor-4.3.0-py2.py3-none-any.whl.metadata (87 kB)\n",
      "Requirement already satisfied: IPython in /opt/tljh/user/lib/python3.12/site-packages (from sglang>=0.5.8->sglang[all]>=0.5.8) (9.9.0)\n",
      "Collecting blobfile==3.0.0 (from sglang>=0.5.8->sglang[all]>=0.5.8)\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting build (from sglang>=0.5.8->sglang[all]>=0.5.8)\n",
      "  Downloading build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cuda-python==12.9 (from sglang>=0.5.8->sglang[all]>=0.5.8)\n",
      "  Downloading cuda_python-12.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting decord2 (from sglang>=0.5.8->sglang[all]>=0.5.8)\n",
      "  Downloading decord2-3.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (686 bytes)\n",
      "Collecting datasets (from sglang>=0.5.8->sglang[all]>=0.5.8)\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting flashinfer_cubin==0.6.1 (from sglang>=0.5.8->sglang[all]>=0.5.8)\n",
      "  Downloading flashinfer_cubin-0.6.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting hf_transfer (from sglang>=0.5.8->sglang[all]>=0.5.8)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "INFO: pip is looking at multiple versions of sglang to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sglang>=0.5.8 (from sglang[all]>=0.5.8)\n",
      "  Downloading sglang-0.5.8-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting vllm>=0.15.0\n",
      "  Downloading vllm-0.15.0-cp38-abi3-manylinux_2_31_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
      "INFO: pip is still looking at multiple versions of sglang to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting setuptools<81.0.0,>=77.0.3 (from vllm>=0.15.0)\n",
      "  Downloading setuptools-80.10.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm>=0.15.0)\n",
      "  Downloading llvmlite-0.44.0rc2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting huggingface-hub<2.0,>=0.16.4 (from tokenizers<=0.23.0,>=0.22.0->transformers)\n",
      "  Downloading huggingface_hub-0.36.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting apache-tvm-ffi!=0.1.8,!=0.1.8.post0,<0.2,>=0.1.6 (from flashinfer-python==0.6.1->vllm>=0.15.0)\n",
      "  Downloading apache_tvm_ffi-0.1.8.post1-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.5-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.10.65-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
      "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "\u001b[31mERROR: Cannot install sglang and vllm==0.15.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    vllm 0.15.0 depends on llguidance<1.4.0 and >=1.3.0; platform_machine == \"x86_64\" or platform_machine == \"arm64\" or platform_machine == \"aarch64\" or platform_machine == \"s390x\" or platform_machine == \"ppc64le\"\n",
      "    sglang 0.5.8.post1 depends on llguidance<0.8.0 and >=0.7.11\n",
      "    sglang 0.5.8 depends on llguidance<0.8.0 and >=0.7.11\n",
      "\n",
      "Additionally, some packages in these conflicts have no matching distributions available for your environment:\n",
      "    llguidance\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate torch 'vllm>=0.15.0' 'sglang[all]>=0.5.8'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f5732f-b290-43a1-9706-be9b5fa56765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "def unload_model(model_name: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Remove model(s) from cache and free GPU memory.\n",
    "    - If model_name is given: unload just that model.\n",
    "    - If model_name is None: unload all cached models.\n",
    "    \"\"\"\n",
    "    global MODEL_CACHE\n",
    "\n",
    "    if model_name is None:\n",
    "        keys = list(MODEL_CACHE.keys())\n",
    "    else:\n",
    "        keys = [model_name] if model_name in MODEL_CACHE else []\n",
    "\n",
    "    for name in keys:\n",
    "        try:\n",
    "            tok, mdl = MODEL_CACHE.pop(name)\n",
    "            del tok\n",
    "            del mdl\n",
    "            print(f\"ðŸ§¹ Unloaded model from memory: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to unload {name}: {e}\")\n",
    "\n",
    "    # Force Python & CUDA to release memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bcaf3a6-b158-4091-aa26-761ff94d6354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF token set in notebook: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ðŸ” Hugging Face access token (READ permission)\n",
    "# IMPORTANT:\n",
    "#  - Use a token that has access to gated models (Llama 3.x)\n",
    "#  - Rotate this token if it was ever shared\n",
    "HF_TOKEN = \"....\"\n",
    "\n",
    "# Make it visible to transformers\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "print(\"HF token set in notebook:\", bool(HF_TOKEN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ccad0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, re, hashlib\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# === Configuration ===\n",
    "DATA_DIR = Path(\"data/6GBench/mcq_questions_only\")\n",
    "SUMMARY_MAX_TURNS = 12\n",
    "\n",
    "# Default model (you can override per run / in cell 7)\n",
    "EVAL_MODEL = \"Qwen/Qwen3-Coder-Next\"\n",
    "\n",
    "# Simple cache so we don't reload weights every call\n",
    "MODEL_CACHE: Dict[str, Tuple[AutoTokenizer, AutoModelForCausalLM]] = {}\n",
    "\n",
    "def get_local_model(model_name: str) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:\n",
    "    \"\"\"\n",
    "    Load a HF model + tokenizer once and cache it.\n",
    "    Uses GPU (A100) via device_map='auto'.\n",
    "    Works for gated models (Llama 3.x) using in-notebook auth.\n",
    "    \"\"\"\n",
    "    if model_name in MODEL_CACHE:\n",
    "        return MODEL_CACHE[model_name]\n",
    "\n",
    "    print(f\"ðŸ”„ Loading model locally: {model_name}\")\n",
    "\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=HF_TOKEN,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=HF_TOKEN,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure pad token exists\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    MODEL_CACHE[model_name] = (tokenizer, model)\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99df20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "TASKS: List[Tuple[str, str, str]] = [\n",
    "  (\n",
    "    \"T1\",\n",
    "    \"Intent Feasibility Assessment\",\n",
    "    \"Given a mission and a 6G intent message, determine whether the intent is feasible under current and near-future network, environmental, and policy constraints. Identify the minimal safe adjustments (e.g., speed, route, slice, autonomy level, sensing configuration) needed to satisfy constraints while preserving mission objectives as much as possible.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T2\",\n",
    "    \"Intent Conflict Resolution\",\n",
    "    \"Resolve conflicts between the operator/mission intent and network or safety policy (e.g., airspace, security, energy, SLA). Decide whether to reject, modify, or conditionally approve the intent, and specify concrete policy-aligned adjustments that balance mission goals and compliance.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T3\",\n",
    "    \"Intent Drift Detection\",\n",
    "    \"Detect subtle changes in mission or network intent over time (e.g., updated priorities, new safety requirements, shifted QoS targets) by comparing past and current intents and behavior. Decide whether the drift is benign, problematic, or requires renegotiation or clarification with other agents or controllers.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T4\",\n",
    "    \"Slice Selection Reasoning\",\n",
    "    \"Given mission requirements and current network telemetry, choose between URLLC, eMBB, or a hybrid slice (or slice configuration) with explicit justification. Trade off latency, reliability, throughput, and robustness, and explain why alternative slices are less appropriate in the given context.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T5\",\n",
    "    \"Slice Switching Decision\",\n",
    "    \"Decide whether to switch, maintain, or augment the current network slice when performance degrades, considering stability, hysteresis, mission criticality, and switching overheads. Prefer decisions that avoid unnecessary oscillations while still preventing SLA violation or safety risk.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T6\",\n",
    "    \"Slice Fairness vs Safety\",\n",
    "    \"Resolve contention for slice resources among multiple agents or swarm members when their demands conflict. Balance fairness, priority levels, and safety margins, possibly degrading some agents more than others, while ensuring global mission safety and compliance with policies.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T7\",\n",
    "    \"Compute Placement Decision\",\n",
    "    \"Choose where to execute AI inference or other compute tasks (onboard, edge, peer, or cloud) under latency, bandwidth, energy, model quality, and trust constraints. Justify placement by considering dynamic network conditions, SLA requirements, and potential failure modes of each location.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T8\",\n",
    "    \"Graceful Degradation under Edge Overload\",\n",
    "    \"When edge resources become overloaded or unstable, decide how to gracefully degrade autonomy or service quality before SLAs are violated. Select which functions to simplify, slow down, or disable while preserving safety-critical behavior and mission viability as long as possible.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T9\",\n",
    "    \"Trust-Aware Offloading\",\n",
    "    \"Evaluate whether to offload tasks or data to edge or third-party compute resources based on trust, security, and policy constraints. Decide when to reject offloading, use partial offloading, or require additional safeguards (e.g., encryption, sandboxing) despite potential performance benefits.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T10\",\n",
    "    \"SLA Violation Prediction\",\n",
    "    \"Predict future SLA violations using early network and system signals such as latency trends, jitter, loss, throughput, edge load, and mission dynamics. Distinguish between transient fluctuations and meaningful trends, and indicate when preemptive mitigation is required to avoid imminent violation.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T11\",\n",
    "    \"Preemptive Autonomy Downgrade\",\n",
    "    \"Before any actual failure or SLA violation occurs, decide when and how to safely downgrade autonomy or functionality based on predicted risk. Choose specific behaviors or capabilities to limit, explaining why the downgrade is justified and how it preserves overall mission safety and compliance.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T12\",\n",
    "    \"Conservative Continuation Decision\",\n",
    "    \"Under uncertainty about network, sensing, or environment, decide whether to continue the mission in a conservative mode or pause/abort. Weigh incomplete or noisy evidence, risk to safety and SLA, and mission criticality, preferring nuanced partial continuation when strictly safe and justifiable.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T13\",\n",
    "    \"Swarm-Level Slice Negotiation\",\n",
    "    \"Coordinate slice allocation across multiple agents or swarm members with competing demands and priorities. Decide how to negotiate and partition slice resources over time, potentially reallocating or renegotiating as conditions change while maintaining global mission performance and fairness.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T14\",\n",
    "    \"Scheduler Reconfiguration Adaptation\",\n",
    "    \"When the underlying AI or network scheduler is reconfigured, updated, or replaced, maintain decision consistency and mission safety. Detect behavioral changes introduced by the new scheduler and adapt policies or intents so that overall system behavior remains coherent and policy-compliant.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T15\",\n",
    "    \"Decision Consistency under Replanning\",\n",
    "    \"Ensure that decisions across multiple planning cycles or turns remain logically consistent with prior commitments, unless new evidence necessitates a change. Avoid contradictory or oscillatory decisions, and when changes are required, justify them with explicit reference to updated context or constraints.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T16\",\n",
    "    \"Network-Exposed Compute Marketplace\",\n",
    "    \"Decide whether, when, and how to expose operator edge/cloud compute resources as a marketplace to third parties under current load, SLAs, and policies. Determine pricing, admission, and allocation strategies that protect critical network services while extracting value from idle capacity.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T17\",\n",
    "    \"Network-Knowledge RAG Augmentation\",\n",
    "    \"Decide what network telemetry, logs, and knowledge to expose to Retrieval-Augmented Generation systems to enhance agent reasoning, under privacy, security, and latency constraints. Balance informativeness against overhead and policy limits, selecting only the most relevant and safe signals.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T18\",\n",
    "    \"AI Agent Identity & Onboarding\",\n",
    "    \"Authorize, authenticate, and register AI agents (device- or network-hosted) and decide how they are represented in identity and access control systems. Define identity mapping, credentials, and onboarding flows that respect policy, security, and interoperability requirements over the agent lifecycle.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T19\",\n",
    "    \"AI Agent Interoperability & Federation\",\n",
    "    \"Resolve compatibility and data-sharing decisions when multiple AI agents from different domains, operators, or networks must collaborate. Decide protocols, translation layers, and data access policies that enable coordination while respecting trust boundaries, privacy, and regulatory constraints.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T20\",\n",
    "    \"Agent-to-Agent Communication Management\",\n",
    "    \"Decide routing, QoS, and security policies for horizontal traffic between AI agents, whether direct or via network relays. Prioritize flows, select paths or slices, and enforce encryption or isolation as needed to meet latency, reliability, and security goals under dynamic network conditions.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T21\",\n",
    "    \"Device-Network Task Offload Arbitration\",\n",
    "    \"Choose whether and how to offload AI or compute tasks from a device to edge, peer, or cloud resources given latency, energy, model capability, and trust constraints. Consider partial offloading, model selection, and fallback strategies, ensuring that decisions remain robust to network fluctuations.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T22\",\n",
    "    \"Federated / Collaborative Learning Orchestration\",\n",
    "    \"Decide when and how to schedule federated or collaborative training and model updates across devices and edge nodes. Respect privacy, regulatory constraints, bandwidth limits, and device heterogeneity, and choose update frequencies and participant sets that balance model quality and resource usage.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T23\",\n",
    "    \"Network-Assisted Digital Twin Control\",\n",
    "    \"Determine how the network should provide sensing, telemetry, and control channels to maintain accurate and actionable real-time digital twins. Decide update rates, data fidelity, and control loop configurations that keep the twin synchronized without overloading the network or violating SLAs.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T24\",\n",
    "    \"Sensing-Enhanced Decisioning (ISAC)\",\n",
    "    \"Choose which sensing streams (e.g., radar, RF, vision, telemetry) and fusion strategies the network should deliver to agents for time-sensitive perception and decision tasks. Trade off sensing accuracy, bandwidth, latency, and robustness, and adapt the sensing configuration as conditions evolve.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T25\",\n",
    "    \"AI-Agent-based Disaster / Public-Safety Coordination\",\n",
    "    \"Coordinate multiple AI agents and UAVs during disaster or public-safety scenarios, deciding slice allocation, sensing priorities, and escalation paths. Balance competing mission goals such as search and rescue, damage assessment, and communication support under extreme and uncertain conditions.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T26\",\n",
    "    \"Trust-Aware Third-Party Agent Exposure\",\n",
    "    \"Decide what level of data, APIs, and compute resources to expose to third-party agents based on trust scores, regulation, and user consent. Enforce differentiated access and isolation policies, and adapt exposure in response to observed behavior, anomalies, or changing regulatory constraints.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T27\",\n",
    "    \"Agent Lifecycle & Management\",\n",
    "    \"Decide lifecycle operations for agents, including instantiation, scaling, migration, upgrade, and retirement, under operator policy and SLA constraints. Coordinate these operations with network load, security posture, and mission demands to avoid service disruption or policy violations.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T28\",\n",
    "    \"6G Model Training-as-a-Service Decision\",\n",
    "    \"Decide when to accept or reject customer requests for network-facilitated model training (e.g., LLM fine-tuning) given resource availability, privacy requirements, and QoS impact. Determine appropriate training configurations, isolation levels, and scheduling so that core network services remain protected.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T29\",\n",
    "    \"Immersive/AR Resource Prioritization\",\n",
    "    \"Allocate slices and edge resources to multi-modal immersive or XR/AR sessions, balancing throughput, latency, stability, and fairness across users and applications. Handle contention by prioritizing critical interactions and gracefully degrading less critical modalities when resources are constrained.\"\n",
    "  ),\n",
    "  (\n",
    "    \"T30\",\n",
    "    \"Network Security Detection & Response Automation\",\n",
    "    \"When AI-driven monitoring flags potential attacks or anomalies, decide automated detection, isolation, mitigation, and recovery actions in the network. Balance swift containment with false-positive risk, and choose responses that preserve critical services and safety while minimizing collateral impact.\"\n",
    "  ),\n",
    "]\n",
    "\n",
    "TASK_MAP: Dict[str, Tuple[str, str]] = {tid: (name, desc) for tid, name, desc in TASKS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d31d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "@dataclass\n",
    "class MCQQuestion:\n",
    "    task_id: str\n",
    "    task_name: str\n",
    "    source_turn: int\n",
    "    question: str\n",
    "    options: Dict[str, str]\n",
    "    correct: str\n",
    "    reason: str\n",
    "    rationale_tag: str\n",
    "    difficulty: str\n",
    "\n",
    "def get_turns(ep: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    return ep.get('dialogue', [])\n",
    "\n",
    "def extract_min_context(ep: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    init_state = ep.get('initial_state', {})\n",
    "    env = init_state.get('env') or {}\n",
    "    airspace = init_state.get('airspace') or {}\n",
    "    uav = init_state.get('uav') or {}\n",
    "    policy = init_state.get('policy') or {}\n",
    "    sensors = (uav.get('sensors') or {})\n",
    "    return {\n",
    "        'env': env,\n",
    "        'airspace': {'alt_bounds': airspace.get('alt_bounds'), 'geofence': airspace.get('geofence')},\n",
    "        'uav': {'pose': uav.get('pose'), 'speed_mps': uav.get('speed_mps'), 'battery_pct': (uav.get('energy') or {}).get('battery_pct'), 'sensors': sensors, 'payloads': uav.get('payloads', [])},\n",
    "        'policy': policy,\n",
    "        'success': ep.get('success'),\n",
    "    }\n",
    "\n",
    "def summarize_episode_for_prompt(ep: Dict[str, Any], max_turns: int = 12) -> str:\n",
    "    ctx = extract_min_context(ep)\n",
    "    parts: List[str] = []\n",
    "    parts.append('Initial context:')\n",
    "    parts.append(f\"- env: {ctx['env']}\")\n",
    "    parts.append(f\"- airspace: {ctx['airspace']}\")\n",
    "    parts.append(f\"- uav: {ctx['uav']}\")\n",
    "    parts.append(f\"- policy: {ctx['policy']}\")\n",
    "    parts.append(f\"- success: {ctx['success']}\")\n",
    "    parts.append('')\n",
    "    parts.append('Dialogue trace (truncated):')\n",
    "    for t in get_turns(ep)[:max_turns]:\n",
    "        turn_no = t.get('turn')\n",
    "        speaker = t.get('speaker')\n",
    "        intent = t.get('intent')\n",
    "        acts = t.get('actions', [])\n",
    "        act_summaries = []\n",
    "        for a in acts:\n",
    "            if a.get('type') == 'mcp':\n",
    "                arg_keys = list((a.get('args') or {}).keys())\n",
    "                act_summaries.append(f\"mcp:{a.get('name')} args_keys={arg_keys}\")\n",
    "            elif a.get('type') == 'a2a':\n",
    "                act_summaries.append(f\"a2a:{a.get('task')} to={a.get('to')}\")\n",
    "        obs = t.get('obs', [])\n",
    "        obs_summaries = []\n",
    "        for o in obs[:3]:\n",
    "            if 'tool' in o:\n",
    "                res = o.get('result', {})\n",
    "                status = res.get('status') if isinstance(res, dict) else None\n",
    "                obs_summaries.append(f\"tool:{o.get('tool')} status={status}\")\n",
    "            elif 'task' in o:\n",
    "                obs_summaries.append(f\"a2a_resp:{o.get('task')} status={o.get('status')}\")\n",
    "        net = t.get('net') or {}\n",
    "        net_s = { 'slice': net.get('slice'), 'lat_ms': net.get('lat_ms'), 'jitter_ms': net.get('jitter_ms'), 'loss_pct': net.get('loss_pct'), 'throughput_mbps': net.get('throughput_mbps'), 'edge_load': net.get('edge_load') }\n",
    "        parts.append(f\"- turn={turn_no} speaker={speaker} intent={intent} actions={act_summaries} obs={obs_summaries} net={net_s}\")\n",
    "    return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8db315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_chat_debug(\n",
    "    model: str,\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.2,\n",
    "    max_tokens: int = 512,  # safer default for max_new_tokens\n",
    "    seed: Optional[int] = None,\n",
    ") -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run a local HF model in chat style and return (completion_text, debug_info).\n",
    "    \"\"\"\n",
    "    tokenizer, lm = get_local_model(model)\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Build a chat prompt\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        chat = [\n",
    "            {\"role\": m.get(\"role\", \"user\"), \"content\": m.get(\"content\", \"\")}\n",
    "            for m in messages\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    else:\n",
    "        # Fallback: simple system + user concatenation\n",
    "        system_parts, user_parts = [], []\n",
    "        for m in messages:\n",
    "            if m[\"role\"] == \"system\":\n",
    "                system_parts.append(m[\"content\"])\n",
    "            else:\n",
    "                user_parts.append(m[\"content\"])\n",
    "        system_text = \"\\n\\n\".join(system_parts).strip()\n",
    "        user_text = \"\\n\\n\".join(user_parts).strip()\n",
    "        prompt = f\"{system_text}\\n\\n{user_text}\" if system_text else user_text\n",
    "\n",
    "    max_ctx = getattr(lm.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "    # First tokenize prompt (truncate if needed)\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_ctx,  # let the tokenizer handle truncation\n",
    "    ).to(lm.device)\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    # Ensure we don't exceed context window\n",
    "    max_new_tokens = min(max_tokens, max_ctx - input_len)\n",
    "    if max_new_tokens <= 0:\n",
    "        # Extreme case: prompt ~max_ctx already\n",
    "        max_new_tokens = 1  # at least try to generate something\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    if pad_id is None:\n",
    "        pad_id = tokenizer.eos_token_id\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": temperature > 0.0,\n",
    "        \"temperature\": temperature,\n",
    "        \"pad_token_id\": pad_id,\n",
    "    }\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    if eos_id is not None:\n",
    "        gen_kwargs[\"eos_token_id\"] = eos_id\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = lm.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    # Slice off the prompt like your Qwen3-Coder-Next example\n",
    "    gen_ids = output_ids[0, input_len:]\n",
    "    completion = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "    debug_info = {\n",
    "        \"prompt\": prompt,\n",
    "        \"input_ids\": inputs[\"input_ids\"][0].tolist(),\n",
    "        \"output_ids\": output_ids[0].tolist(),\n",
    "        \"max_ctx\": max_ctx,\n",
    "        \"used_max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "    return completion.strip(), debug_info\n",
    "\n",
    "\n",
    "def build_eval_messages(\n",
    "    task_id: str,\n",
    "    episode_summary: str,\n",
    "    question: Dict[str, Any],\n",
    ") -> List[Dict[str, str]]:\n",
    "    task_name, task_def = TASK_MAP[task_id]\n",
    "    q_text = question['question']\n",
    "    options = question['options']\n",
    "\n",
    "    user_content: List[str] = []\n",
    "    user_content.append('TARGET TASK')\n",
    "    user_content.append(f'TASK_ID: {task_id}')\n",
    "    user_content.append(f'TASK_NAME: {task_name}')\n",
    "    user_content.append(f'DEFINITION: {task_def}')\n",
    "    user_content.append('')\n",
    "    user_content.append('EPISODE SUMMARY')\n",
    "    user_content.append(episode_summary)\n",
    "    user_content.append('')\n",
    "    user_content.append('QUESTION')\n",
    "    user_content.append(q_text)\n",
    "    user_content.append('')\n",
    "    user_content.append('OPTIONS')\n",
    "    for k in ['A', 'B', 'C', 'D']:\n",
    "        user_content.append(f\"{k}: {options[k]}\")\n",
    "    user_content.append('')\n",
    "    user_content.append(\n",
    "        'Respond ONLY with valid JSON of the form {\"answer\": \"A/B/C/D\"} where answer is one of \"A\", \"B\", \"C\", or \"D\".'\n",
    "    )\n",
    "\n",
    "    system_msg = (\n",
    "        'You are an expert 6G network AI agent evaluator. '\n",
    "        'You will answer a multiple-choice question (A/B/C/D) about a UAV mission episode. '\n",
    "        'Use the episode context and the target task definition to choose the best answer. '\n",
    "        'You MUST respond with a single JSON object of the form {\"answer\": \"A\" or \"B\" or \"C\" or \"D\"}.'\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {'role': 'system', 'content': system_msg},\n",
    "        {'role': 'user', 'content': '\\n'.join(user_content)},\n",
    "    ]\n",
    "\n",
    "\n",
    "def parse_mcq_answer_from_json(raw) -> Optional[str]:\n",
    "    if isinstance(raw, dict):\n",
    "        obj = raw\n",
    "    else:\n",
    "        try:\n",
    "            obj = json.loads(raw)\n",
    "        except Exception:\n",
    "            obj = None\n",
    "    if isinstance(obj, dict):\n",
    "        for key in ['answer', 'choice', 'label', 'option']:\n",
    "            val = obj.get(key)\n",
    "            if isinstance(val, str) and val.strip() in {'A', 'B', 'C', 'D'}:\n",
    "                return val.strip()\n",
    "\n",
    "    text = raw if isinstance(raw, str) else str(raw)\n",
    "    m = re.search(r'\"answer\"\\s*:\\s*\"([ABCD])\"', text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m2 = re.search(r'\\b([ABCD])\\b', text)\n",
    "    if m2:\n",
    "        return m2.group(1)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a9f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_episode_mcq_pairs(data_dir: Path) -> List[Tuple[str, Dict[str, Any], Dict[str, Any]]]:\n",
    "    episodes: Dict[str, Path] = {}\n",
    "    mcqs: Dict[str, Path] = {}\n",
    "\n",
    "    # Collect all episode and MCQ files\n",
    "    for p in data_dir.glob('*.episode.json'):\n",
    "        base = p.name.replace('.episode.json', '')\n",
    "        episodes[base] = p\n",
    "\n",
    "    for p in data_dir.glob('*.mcq.json'):\n",
    "        base = p.name.replace('.mcq.json', '')\n",
    "        mcqs[base] = p\n",
    "\n",
    "    # Only keep keys that have both episode and mcq\n",
    "    common_keys = sorted(set(episodes.keys()) & set(mcqs.keys()))\n",
    "    pairs: List[Tuple[str, Dict[str, Any], Dict[str, Any]]] = []\n",
    "\n",
    "    for key in common_keys:\n",
    "        with episodes[key].open('r', encoding='utf-8') as f_ep:\n",
    "            ep = json.load(f_ep)\n",
    "        with mcqs[key].open('r', encoding='utf-8') as f_q:\n",
    "            mcq = json.load(f_q)\n",
    "        episode_id = mcq.get('episode_id', key)\n",
    "        pairs.append((episode_id, ep, mcq))\n",
    "\n",
    "    print(f'Found {len(pairs)} episode/MCQ pairs in {data_dir}')\n",
    "    return pairs\n",
    "\n",
    "def eval_model_on_dir(\n",
    "    data_dir: Path,\n",
    "    model: str,\n",
    "    max_pairs: Optional[int] = None,\n",
    "    seed_base: Optional[int] = 42,\n",
    ") -> Dict[str, Any]:\n",
    "    # KEEP using episode_mcq_pairs (unchanged)\n",
    "    pairs = load_episode_mcq_pairs(data_dir)\n",
    "    if max_pairs is not None:\n",
    "        pairs = pairs[:max_pairs]\n",
    "    # --- count total questions across all episodes ---\n",
    "    total_questions_all = sum(\n",
    "        len(mcq.get(\"questions\", [])) for _, _, mcq in pairs\n",
    "    )\n",
    "\n",
    "    avg_q_per_episode = (\n",
    "        total_questions_all / len(pairs) if pairs else 0.0\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Total episodes: {len(pairs)} | \"\n",
    "        f\"Total questions (all episodes): {total_questions_all} | \"\n",
    "        f\"Avg questions/episode: {avg_q_per_episode:.2f}\"\n",
    "    )\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    per_task_stats: Dict[str, Dict[str, int]] = {}\n",
    "    per_question_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Text progress bar (works well in VS Code)\n",
    "    pbar = tqdm(\n",
    "        total=len(pairs),\n",
    "        desc=\"Eval episodes\",\n",
    "        unit=\"ep\",\n",
    "    )\n",
    "\n",
    "    for episode_id, ep, mcq in pairs:\n",
    "        episode_summary = summarize_episode_for_prompt(ep, max_turns=SUMMARY_MAX_TURNS)\n",
    "        questions = mcq.get('questions', [])\n",
    "\n",
    "        for q in questions:\n",
    "            task_id = q['task_id']\n",
    "            gold = q['correct']\n",
    "\n",
    "            messages = build_eval_messages(task_id, episode_summary, q)\n",
    "\n",
    "            seed = None\n",
    "            if seed_base is not None:\n",
    "                s = f\"{seed_base}:{episode_id}:{task_id}\".encode('utf-8')\n",
    "                seed = int(hashlib.sha256(s).hexdigest()[:8], 16)\n",
    "\n",
    "            raw_text, api_parsed = local_chat_debug(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "                max_tokens=10000,  # adjust if you need longer answers\n",
    "                seed=seed,\n",
    "            )\n",
    "\n",
    "\n",
    "            extracted_content = None\n",
    "            if isinstance(api_parsed, dict):\n",
    "                try:\n",
    "                    extracted_content = api_parsed['choices'][0]['message']['content']\n",
    "                except Exception:\n",
    "                    extracted_content = None\n",
    "\n",
    "            parse_input = extracted_content if extracted_content is not None else raw_text\n",
    "            pred = parse_mcq_answer_from_json(parse_input)\n",
    "\n",
    "            is_correct = int(pred == gold)\n",
    "            total += 1\n",
    "            correct += is_correct\n",
    "\n",
    "            stats = per_task_stats.setdefault(task_id, {'total': 0, 'correct': 0})\n",
    "            stats['total'] += 1\n",
    "            stats['correct'] += is_correct\n",
    "\n",
    "            per_question_records.append({\n",
    "                'episode_id': episode_id,\n",
    "                'task_id': task_id,\n",
    "                'task_name': q.get('task_name'),\n",
    "                'difficulty': q.get('difficulty'),\n",
    "                'pred': pred,\n",
    "                'gold': gold,\n",
    "                'correct_flag': is_correct,\n",
    "                'raw_model_output': raw_text,\n",
    "                'api_parsed_json': api_parsed,\n",
    "                'extracted_content': extracted_content,\n",
    "                'question': q.get('question'),\n",
    "                'options': q.get('options'),\n",
    "                'source_turn': q.get('source_turn'),\n",
    "            })\n",
    "\n",
    "        # === REAL-TIME METRICS ===\n",
    "        incorrect = total - correct\n",
    "        current_acc = correct / total if total else 0.0\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(\n",
    "            q=total,\n",
    "            ok=correct,\n",
    "            wrong=incorrect,\n",
    "            acc=f\"{current_acc:.3f}\",\n",
    "        )\n",
    "\n",
    "        tqdm.write(\n",
    "            f\"[running] episodes={pbar.n}/{pbar.total} | \"\n",
    "            f\"questions={total} | overall_acc={current_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    overall_acc = correct / total if total else 0.0\n",
    "    per_task_acc = {\n",
    "        tid: s['correct'] / s['total'] if s['total'] else 0.0\n",
    "        for tid, s in per_task_stats.items()\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'per_task_accuracy': per_task_acc,\n",
    "        'total_questions': total,\n",
    "        'records': per_question_records,\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dea7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_task_question_counts(data_dir: Path) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Print how many MCQ questions exist per task across all episodes.\n",
    "    Returns a dict {task_id: count}.\n",
    "    \"\"\"\n",
    "    pairs = load_episode_mcq_pairs(data_dir)\n",
    "\n",
    "    task_counts: Dict[str, int] = {}\n",
    "    total_questions = 0\n",
    "\n",
    "    for _, _, mcq in pairs:\n",
    "        for q in mcq.get(\"questions\", []):\n",
    "            task_id = q.get(\"task_id\", \"UNKNOWN\")\n",
    "            task_counts[task_id] = task_counts.get(task_id, 0) + 1\n",
    "            total_questions += 1\n",
    "\n",
    "    print(\"\\nQuestion count per task:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for task_id, count in sorted(task_counts.items()):\n",
    "        task_name, _ = TASK_MAP.get(task_id, (\"?\", \"\"))\n",
    "        print(f\"{task_id:>3} | {task_name:<45} | {count:5d}\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"TOTAL QUESTIONS: {total_questions}\")\n",
    "\n",
    "    return task_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f434e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 488 episode/MCQ pairs in data/6GBench/mcq_questions_only\n",
      "\n",
      "Question count per task:\n",
      "------------------------------------------------------------\n",
      " T1 | Intent Feasibility Assessment                 |   115\n",
      "T10 | SLA Violation Prediction                      |   115\n",
      "T11 | Preemptive Autonomy Downgrade                 |   119\n",
      "T12 | Conservative Continuation Decision            |   107\n",
      "T13 | Swarm-Level Slice Negotiation                 |   128\n",
      "T14 | Scheduler Reconfiguration Adaptation          |   107\n",
      "T15 | Decision Consistency under Replanning         |   129\n",
      "T16 | Network-Exposed Compute Marketplace           |   155\n",
      "T17 | Network-Knowledge RAG Augmentation            |   154\n",
      "T18 | AI Agent Identity & Onboarding                |   157\n",
      "T19 | AI Agent Interoperability & Federation        |   129\n",
      " T2 | Intent Conflict Resolution                    |   113\n",
      "T20 | Agent-to-Agent Communication Management       |   105\n",
      "T21 | Device-Network Task Offload Arbitration       |   132\n",
      "T22 | Federated / Collaborative Learning Orchestration |   144\n",
      "T23 | Network-Assisted Digital Twin Control         |   114\n",
      "T24 | Sensing-Enhanced Decisioning (ISAC)           |   138\n",
      "T25 | AI-Agent-based Disaster / Public-Safety Coordination |   112\n",
      "T26 | Trust-Aware Third-Party Agent Exposure        |   142\n",
      "T27 | Agent Lifecycle & Management                  |   106\n",
      "T28 | 6G Model Training-as-a-Service Decision       |   114\n",
      "T29 | Immersive/AR Resource Prioritization          |   146\n",
      " T3 | Intent Drift Detection                        |   112\n",
      "T30 | Network Security Detection & Response Automation |   145\n",
      " T4 | Slice Selection Reasoning                     |   100\n",
      " T5 | Slice Switching Decision                      |   118\n",
      " T6 | Slice Fairness vs Safety                      |   104\n",
      " T7 | Compute Placement Decision                    |   117\n",
      " T8 | Graceful Degradation under Edge Overload      |   130\n",
      " T9 | Trust-Aware Offloading                        |   115\n",
      "------------------------------------------------------------\n",
      "TOTAL QUESTIONS: 3722\n"
     ]
    }
   ],
   "source": [
    "task_counts = print_task_question_counts(DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "622f9f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 488 episode/MCQ pairs in data/6GBench/mcq_questions_only\n",
      "--- Diagnostic: prompt length ---\n",
      "5681 chars\n",
      "ðŸ”„ Loading model locally: Qwen/Qwen3-Coder-Next\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb91822294d40d79940f91113d366d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ced23abefc4d2f990f8a1f3cff414b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ef8d62ab454d9889d1404069210d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a239cc29657e4d2dba34254636098380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8096a6e818f3435681de7da207ff5c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a7ef9306404b53895613cd632eda6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faba70e66f9a4eeab21351dacf55a1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7023910094ad487d988535f0c5f33ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65b9420a2784b7a8f63b2667020c141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of the required library is not installed. Falling back to torch implementation. To install follow https://github.com/fla-org/flash-linear-attention#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb58c2a4b844d0d9f60ef579f804507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4647a41d8780436d9e2e247bf3f0ab68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Completion (first 2000 chars) ---\n",
      "{\"answer\": \"C\"}\n",
      "\n",
      "--- Parsed answer ---\n",
      "pred = C\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC: run one local request and inspect output\n",
    "pairs = load_episode_mcq_pairs(DATA_DIR)\n",
    "if len(pairs) == 0:\n",
    "    raise RuntimeError(f'No episode/mcq pairs found in {DATA_DIR}')\n",
    "\n",
    "episode_id, ep, mcq = pairs[0]\n",
    "q = mcq['questions'][0]\n",
    "task_id = q['task_id']\n",
    "\n",
    "messages = build_eval_messages(\n",
    "    task_id,\n",
    "    summarize_episode_for_prompt(ep, SUMMARY_MAX_TURNS),\n",
    "    q\n",
    ")\n",
    "\n",
    "print('--- Diagnostic: prompt length ---')\n",
    "print(sum(len(m[\"content\"]) for m in messages), \"chars\")\n",
    "\n",
    "raw_text, debug_info = local_chat_debug(\n",
    "    model=EVAL_MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    "    max_tokens=10000,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print('\\n--- Completion (first 2000 chars) ---')\n",
    "print(raw_text[:2000])\n",
    "\n",
    "print('\\n--- Parsed answer ---')\n",
    "print(\"pred =\", parse_mcq_answer_from_json(raw_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Eval episodes:   0%|          | 0/488 [00:00<?, ?ep/s]\u001b[A\n",
      "Eval episodes:   0%|          | 1/488 [00:44<5:58:34, 44.18s/ep]\u001b[A\n",
      "Eval episodes:   0%|          | 1/488 [00:44<5:58:34, 44.18s/ep, acc=0.778, ok=7, q=9, wrong=2]\u001b[A\n",
      "                                                                                               \n",
      "Evaluating models:   0%|          | 0/1 [00:45<?, ?it/s].18s/ep, acc=0.778, ok=7, q=9, wrong=2]\u001b[A"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "def format_seconds(sec: float) -> str:\n",
    "    return str(timedelta(seconds=int(sec)))\n",
    "\n",
    "EVAL_MODELS = [\n",
    "    \"Qwen/Qwen3-Coder-Next\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    # \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "]\n",
    "\n",
    "TXT_LOG_PATH = Path(\n",
    "    \"data/6GBench/\"\n",
    "    \"6gbench_mcq_eval_cell_output_Qwen_qwen3_coder_next.txt\"\n",
    ")\n",
    "\n",
    "TIME_LOG_PATH = Path(\n",
    "    \"data/6GBench/\"\n",
    "    \"6gbench_mcq_eval_timing_Qwen_qwen3_coder_next.txt\"\n",
    ")\n",
    "\n",
    "global_start_time = time.time()\n",
    "per_model_times = {}\n",
    "\n",
    "with TXT_LOG_PATH.open(\"a\", encoding=\"utf-8\") as log_file:\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = log_file  # redirect ALL prints to file\n",
    "\n",
    "    try:\n",
    "        for EVAL_MODEL in tqdm(EVAL_MODELS, desc=\"Evaluating models\"):\n",
    "\n",
    "            model_start_time = time.time()\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"Evaluating model: {EVAL_MODEL}\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "            # Run evaluation (all its prints are captured)\n",
    "            results = eval_model_on_dir(\n",
    "                DATA_DIR,\n",
    "                EVAL_MODEL,\n",
    "                max_pairs=None,\n",
    "                seed_base=42\n",
    "            )\n",
    "\n",
    "            print(f\"Model: {results['model']}\")\n",
    "            print(f\"Total questions: {results['total_questions']}\")\n",
    "            print(f\"Overall accuracy: {results['overall_accuracy']:.3f}\")\n",
    "\n",
    "            print(\"\\nPer-task accuracy:\")\n",
    "            for tid, acc in sorted(results['per_task_accuracy'].items()):\n",
    "                tname, _ = TASK_MAP.get(tid, ('?', ''))\n",
    "                print(f\"  {tid} ({tname}): {acc:.3f}\")\n",
    "\n",
    "            recs = results['records']\n",
    "            none_preds = sum(1 for r in recs if r['pred'] is None)\n",
    "            print(\n",
    "                f\"\\nParsed predictions for {len(recs) - none_preds}/{len(recs)} questions.\"\n",
    "            )\n",
    "\n",
    "            # ----- Save global results -----\n",
    "            safe_model_name = EVAL_MODEL.replace(\"/\", \"_\")\n",
    "            out_path = Path(f\"6gbench_mcq_eval_results_{safe_model_name}_debug.json\")\n",
    "\n",
    "            with out_path.open('w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            print(f'Saved detailed global results to {out_path.resolve()}')\n",
    "\n",
    "            # ----- Save per-task results -----\n",
    "            per_task_dir = Path(\n",
    "                \"data/6GBench/mcq_eval_per_task_debug\"\n",
    "            )\n",
    "            per_task_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            per_task_records = {}\n",
    "            for r in results['records']:\n",
    "                per_task_records.setdefault(r['task_id'], []).append(r)\n",
    "\n",
    "            for tid, task_recs in per_task_records.items():\n",
    "                tname, _ = TASK_MAP.get(tid, ('?', ''))\n",
    "                safe_name = tname.replace(' ', '_').replace('/', '_')\n",
    "                fname = per_task_dir / f\"{safe_model_name}_{tid}_{safe_name}.json\"\n",
    "\n",
    "                with fname.open('w', encoding='utf-8') as f:\n",
    "                    json.dump(task_recs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "                print(f'Saved {len(task_recs)} records for {tid} ({tname}) to {fname}')\n",
    "\n",
    "            # ----- Per-model timing -----\n",
    "            model_elapsed = time.time() - model_start_time\n",
    "            per_model_times[EVAL_MODEL] = model_elapsed\n",
    "\n",
    "            print(\n",
    "                f\"\\nâ± Model runtime: {format_seconds(model_elapsed)} \"\n",
    "                f\"({model_elapsed:.1f} seconds)\"\n",
    "            )\n",
    "\n",
    "            # ----- NEW: unload this model and free GPU memory -----\n",
    "            unload_model(EVAL_MODEL)\n",
    "            print(f\"ðŸ§¹ Freed GPU memory for {EVAL_MODEL}\")\n",
    "\n",
    "    finally:\n",
    "        sys.stdout = old_stdout  # always restore stdout\n",
    "\n",
    "global_elapsed = time.time() - global_start_time\n",
    "\n",
    "with TIME_LOG_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"6G-Bench MCQ Evaluation Timing Report\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "    for model, sec in per_model_times.items():\n",
    "        f.write(f\"Model: {model}\\n\")\n",
    "        f.write(f\"  Time: {format_seconds(sec)} ({sec:.1f} seconds)\\n\\n\")\n",
    "\n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    f.write(\n",
    "        f\"TOTAL EXPERIMENT TIME: \"\n",
    "        f\"{format_seconds(global_elapsed)} \"\n",
    "        f\"({global_elapsed:.1f} seconds)\\n\"\n",
    "    )\n",
    "\n",
    "print(f\"âœ” Full cell output appended to {TXT_LOG_PATH.resolve()}\")\n",
    "print(f\"âœ” Timing report saved to {TIME_LOG_PATH.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
